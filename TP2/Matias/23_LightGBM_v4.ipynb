{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(\"../data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(x,char):\n",
    "    words = \"\"\n",
    "    for word in x:\n",
    "        if word.startswith(char):\n",
    "            words = words + word + \" \"\n",
    "    return words\n",
    "\n",
    "def count_vowels(x):\n",
    "    return (x.count('a') + x.count('e') + x.count('i') + x.count('o') + x.count('u'))\n",
    "\n",
    "def count_short_words(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if 1 <= len(word) <= 3:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_stopwords(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):        \n",
    "    clean_word = ''.join([char for char in word if char not in string.punctuation])\n",
    "    return clean_word\n",
    "\n",
    "def cleaning_text(text):\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    text_tokenize = tokenizer.tokenize(text)\n",
    "    wordlist = []\n",
    "    for word in text_tokenize:\n",
    "        word = word.lower()\n",
    "        word = re.sub('(?P<url>https?://[^\\s]+)', ' ', word)\n",
    "        word = remove_punctuation(word)\n",
    "        word = re.sub(r'[^\\w]', ' ', word)\n",
    "        word = unidecode.unidecode(word)\n",
    "        word = re.sub(r'[0-9]','', word)\n",
    "        if((word != '')&(word != ' ')&(word not in stopwords)):\n",
    "            wordlist.append(word)\n",
    "    clean_text = ' '.join(wordlist)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target_label</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>68</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>130</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#wildfires</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>56</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>85</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this earthquake ma...   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  all residents asked to shelter in place are be...   \n",
       "3   6     NaN      NaN  people receive wildfires evacuation orders in ...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby alaska as s...   \n",
       "\n",
       "   target_label  special_chars_count             hashtags labels  \\\n",
       "0             1                    1         #earthquake           \n",
       "1             1                    1                               \n",
       "2             1                    3                               \n",
       "3             1                    2          #wildfires           \n",
       "4             1                    2  #alaska #wildfires           \n",
       "\n",
       "   hashtags_count  labels_count  num_chars_count  \\\n",
       "0               1             0                0   \n",
       "1               0             0                0   \n",
       "2               0             0                0   \n",
       "3               1             0                5   \n",
       "4               2             0                0   \n",
       "\n",
       "                                          clean_text  text_length  \\\n",
       "0       deeds reason earthquake may allah forgive us           68   \n",
       "1              forest fire near la ronge sask canada           37   \n",
       "2  residents asked shelter place notified officer...          130   \n",
       "3  people receive wildfires evacuation orders cal...           56   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...           85   \n",
       "\n",
       "   vowels_count  short_words_count  stopwords_count  words_count  \n",
       "0            25                  7                6           13  \n",
       "1            13                  1                0            7  \n",
       "2            45                  9               11           22  \n",
       "3            24                  1                1            7  \n",
       "4            25                  3                7           16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[\"special_chars_count\"] =  tweets[\"text\"]\n",
    "tweets[\"special_chars_count\"] =  tweets[\"special_chars_count\"].str.lower()\n",
    "tweets[\"special_chars_count\"] = tweets[\"special_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "tweets[\"special_chars_count\"] = tweets[\"special_chars_count\"].str.strip()\n",
    "tweets[\"special_chars_count\"] = tweets[\"special_chars_count\"].apply(lambda x: re.sub(' +','', x))\n",
    "tweets[\"special_chars_count\"] = tweets[\"special_chars_count\"].apply(lambda x: re.sub(r'[0-9]','', x))\n",
    "tweets[\"special_chars_count\"] = tweets[\"special_chars_count\"].str.len()\n",
    "\n",
    "tweets[\"hashtags\"] = tweets[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'#'))\n",
    "tweets[\"labels\"] = tweets[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'@'))\n",
    "tweets[\"hashtags_count\"] = tweets[\"hashtags\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "tweets[\"labels_count\"] = tweets[\"labels\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "\n",
    "tweets[\"num_chars_count\"] = tweets[\"text\"]\n",
    "tweets[\"num_chars_count\"] =  tweets[\"num_chars_count\"].str.lower()\n",
    "tweets[\"num_chars_count\"] = tweets[\"num_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "tweets[\"num_chars_count\"] = tweets[\"num_chars_count\"].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "tweets[\"num_chars_count\"] = tweets[\"num_chars_count\"].str.strip()\n",
    "tweets[\"num_chars_count\"] = tweets[\"num_chars_count\"].str.len()\n",
    "\n",
    "tweets[\"clean_text\"] = tweets[\"text\"].apply(lambda x: cleaning_text(x))\n",
    "\n",
    "tweets[\"text\"] = tweets[\"text\"].str.lower()\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub('(?P<url>https?://[^\\s]+)', ' ', x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'_', ' ', x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'[0-9]',' ', x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(' +',' ', x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: unidecode.unidecode(x))\n",
    "tweets[\"text\"] = tweets[\"text\"].str.strip()\n",
    "tweets[\"text_length\"] = tweets[\"text\"].str.len()\n",
    "\n",
    "tweets[\"vowels_count\"] = tweets[\"text\"].apply(lambda x: count_vowels(x))\n",
    "tweets[\"short_words_count\"] = tweets[\"text\"].apply(lambda x: count_short_words(x))\n",
    "tweets[\"stopwords_count\"] = tweets[\"text\"].apply(lambda x: count_stopwords(x))\n",
    "tweets[\"text\"] = tweets[\"text\"].apply(lambda x: re.sub(r'\\b\\w{1}\\b', '', x))\n",
    "tweets[\"words_count\"] = tweets[\"text\"].str.split(' ').apply(lambda x: len(x))\n",
    "\n",
    "tweets.rename(columns={\"target\":\"target_label\"}, inplace=True)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"keyword\"] = tweets[\"keyword\"].str.replace('%20',' ')\n",
    "tweets[\"keyword\"] = tweets[\"keyword\"].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['keyword_ablaze', 'keyword_accident', 'keyword_aftershock',\n",
       "       'keyword_airplane accident', 'keyword_ambulance', 'keyword_annihilated',\n",
       "       'keyword_annihilation', 'keyword_apocalypse', 'keyword_armageddon',\n",
       "       'keyword_army',\n",
       "       ...\n",
       "       'keyword_weapons', 'keyword_whirlwind', 'keyword_wild fires',\n",
       "       'keyword_wildfire', 'keyword_windstorm', 'keyword_wounded',\n",
       "       'keyword_wounds', 'keyword_wreck', 'keyword_wreckage',\n",
       "       'keyword_wrecked'],\n",
       "      dtype='object', length=221)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one hot encoding\n",
    "dummies = pd.get_dummies(tweets[\"keyword\"], prefix=\"keyword\")\n",
    "dummies.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 238)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_ohe = pd.concat([tweets,dummies], axis=\"columns\")\n",
    "tweets_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BOW\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(tweets[\"clean_text\"])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaalll</th>\n",
       "      <th>aaarrrgghhh</th>\n",
       "      <th>aaemiddleaged</th>\n",
       "      <th>aal</th>\n",
       "      <th>aan</th>\n",
       "      <th>aannnd</th>\n",
       "      <th>aar</th>\n",
       "      <th>...</th>\n",
       "      <th>zones</th>\n",
       "      <th>zonewolf</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zotar</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zrnf</th>\n",
       "      <th>zss</th>\n",
       "      <th>zumiez</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 14190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aa  aaa  aaaand  aaalll  aaarrrgghhh  aaemiddleaged  aal  aan  aannnd  aar  \\\n",
       "0   0    0       0       0            0              0    0    0       0    0   \n",
       "1   0    0       0       0            0              0    0    0       0    0   \n",
       "2   0    0       0       0            0              0    0    0       0    0   \n",
       "3   0    0       0       0            0              0    0    0       0    0   \n",
       "4   0    0       0       0            0              0    0    0       0    0   \n",
       "\n",
       "   ...  zones  zonewolf  zoom  zotar  zouma  zrnf  zss  zumiez  zurich  zzz  \n",
       "0  ...      0         0     0      0      0     0    0       0       0    0  \n",
       "1  ...      0         0     0      0      0     0    0       0       0    0  \n",
       "2  ...      0         0     0      0      0     0    0       0       0    0  \n",
       "3  ...      0         0     0      0      0     0    0       0       0    0  \n",
       "4  ...      0         0     0      0      0     0    0       0       0    0  \n",
       "\n",
       "[5 rows x 14190 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_words = vectorizer.get_feature_names()\n",
    "df_words = pd.DataFrame(X.toarray(), columns=feature_words)\n",
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2080)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filter = df_words.loc[:,(df_words.sum()>5)]\n",
    "df_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2318)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_final = pd.concat([tweets_ohe,df_filter], axis=\"columns\")\n",
    "tweets_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_final.drop([\"id\",\"keyword\",\"location\",\"text\",\"target_label\",\"hashtags\",\"labels\",\"clean_text\"], axis=1)\n",
    "y = tweets_final[\"target_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 2308)\n",
      "(7613,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando las 2308 features\n",
    "model_lgb = lgb.LGBMClassifier()\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.785464\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_importances = pd.DataFrame(model_lgb.feature_importances_, index=X_train.columns, columns=[\"importancia\"]).\\\n",
    "        sort_values(by=\"importancia\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special_chars_count</th>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vowels_count</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords_count</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_words_count</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     importancia\n",
       "text_length                  102\n",
       "special_chars_count           88\n",
       "vowels_count                  84\n",
       "stopwords_count               70\n",
       "short_words_count             57"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_length', 'special_chars_count', 'vowels_count', 'stopwords_count',\n",
       "       'short_words_count', 'num_chars_count', 'words_count', 'hiroshima',\n",
       "       'labels_count', 'fires',\n",
       "       ...\n",
       "       'uam', 'typhoondevastated', 'type', 'twister', 'twice', 'tweets',\n",
       "       'tweet', 'tv', 'turn', 'uiwhen'],\n",
       "      dtype='object', length=800)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fi = df_feat_importances.index[:800].tolist()\n",
    "X = X.filter(items=list_fi)\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=21,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=53, objective='binary',\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando 800 features\n",
    "model_lgb = lgb.LGBMClassifier(n_estimators=100, colsample_bytree=0.5, num_leaves=53, objective='binary', max_depth=21)\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.790280\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.7,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=7,\n",
       "               min_child_samples=20, min_child_weight=0.05, min_split_gain=0.0,\n",
       "               n_estimators=500, n_jobs=-1, num_leaves=14, objective='binary',\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb = lgb.LGBMClassifier(n_estimators=500, colsample_bytree=0.7, num_leaves=14, objective='binary', max_depth=7,\n",
    "                              min_child_samples=20, min_child_weight=0.05)\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.795972\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ahora con tf-idf\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(tweets[\"clean_text\"])\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2072)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words = pd.DataFrame(X.toarray(), columns=feature_words)\n",
    "df_filter = df_words.loc[:,(df_words.sum()>2)]\n",
    "df_filter.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2310)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_final = pd.concat([tweets_ohe,df_filter], axis=\"columns\")\n",
    "tweets_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets_final.drop([\"id\",\"keyword\",\"location\",\"text\",\"target_label\",\"hashtags\",\"labels\",\"clean_text\"], axis=1)\n",
    "y = tweets_final[\"target_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 2300)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando las 2300 features\n",
    "model_lgb = lgb.LGBMClassifier()\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.781524\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_importances = pd.DataFrame(model_lgb.feature_importances_, index=X_train.columns, columns=[\"importancia\"]).\\\n",
    "        sort_values(by=\"importancia\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1931\n",
       "1       43\n",
       "2       34\n",
       "7       33\n",
       "6       31\n",
       "3       31\n",
       "4       29\n",
       "5       27\n",
       "8       20\n",
       "11      17\n",
       "9       16\n",
       "10      16\n",
       "14      13\n",
       "12      10\n",
       "13       7\n",
       "17       6\n",
       "15       5\n",
       "16       4\n",
       "18       4\n",
       "26       4\n",
       "21       3\n",
       "23       2\n",
       "25       2\n",
       "19       2\n",
       "47       2\n",
       "31       1\n",
       "35       1\n",
       "60       1\n",
       "30       1\n",
       "54       1\n",
       "70       1\n",
       "27       1\n",
       "69       1\n",
       "Name: importancia, dtype: int64"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_importances[\"importancia\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2300-1931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 800)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fi = df_feat_importances.index[:800].tolist()\n",
    "X = X.filter(items=list_fi)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importancia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vowels_count</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>special_chars_count</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopwords_count</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_chars_count</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>short_words_count</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hiroshima</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fires</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labels_count</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>people</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words_count</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earthquake</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>storm</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disaster</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>killed</th>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>car</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>japan</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wildfire</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword_casualties</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtags_count</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debris</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spill</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massacre</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migrants</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explosion</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     importancia\n",
       "vowels_count                  70\n",
       "text_length                   69\n",
       "special_chars_count           60\n",
       "stopwords_count               54\n",
       "num_chars_count               47\n",
       "short_words_count             47\n",
       "hiroshima                     35\n",
       "fires                         31\n",
       "labels_count                  30\n",
       "people                        27\n",
       "near                          26\n",
       "words_count                   26\n",
       "earthquake                    26\n",
       "storm                         26\n",
       "buildings                     25\n",
       "train                         25\n",
       "disaster                      23\n",
       "killed                        23\n",
       "car                           21\n",
       "new                           21\n",
       "suicide                       21\n",
       "japan                         19\n",
       "wildfire                      19\n",
       "keyword_casualties            18\n",
       "hashtags_count                18\n",
       "debris                        18\n",
       "spill                         18\n",
       "massacre                      17\n",
       "migrants                      17\n",
       "explosion                     17"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_importances.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.6,\n",
       "               importance_type='split', learning_rate=0.2, max_depth=7,\n",
       "               min_child_samples=10, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=150, n_jobs=1, num_leaves=14, objective='binary',\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=1, silent=True,\n",
       "               subsample=1, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando 800 features\n",
    "model_lgb = lgb.LGBMClassifier(num_leaves=14, objective=\"binary\", colsample_bytree=0.6, subsample=1, max_depth=7, n_jobs=1,\n",
    "                              n_estimators=150, min_child_samples=10, reg_lambda=1, learning_rate=0.2)\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.791594\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 70, 77, 68, 53, 65, 15, 18, 21, 13, 11, 49, 11, 15, 11, 11, 15,\n",
       "        9,  9,  9, 12,  8,  8,  9,  5,  8,  9,  9,  8,  8,  6,  5,  9,  9,\n",
       "        6,  5,  7, 11,  9, 10,  9,  9,  6,  5,  6,  6,  7,  8,  8,  4,  0,\n",
       "        7,  6,  8,  4,  8,  6,  5,  7,  5,  7,  7,  9,  6,  5,  7,  8,  7,\n",
       "        4,  7,  5,  6,  0,  5,  5, 10,  5,  5,  4,  4,  7,  2,  4,  5,  4,\n",
       "        6,  4,  5,  5,  4,  4,  4,  4,  3,  4,  7,  3,  3,  5,  3,  4,  4,\n",
       "        2, 10,  5,  3,  3,  4,  2,  4,  5,  5,  5,  4,  4,  4,  1,  3,  1,\n",
       "        4,  3,  5,  2,  2,  6,  3,  4,  1,  4,  1,  2,  2,  3,  3,  3,  2,\n",
       "        4,  1,  5,  2,  3,  2,  2,  3,  4,  1,  3,  2,  3,  3,  3,  1,  2,\n",
       "        4,  7,  3,  6,  3,  4,  3,  4,  3,  6,  3,  3,  4,  1,  4,  2,  4,\n",
       "        3,  3,  3,  4,  0,  2,  1,  1,  2,  3,  4,  2,  1,  3,  3,  3,  5,\n",
       "        0,  1,  1,  5,  3,  1,  0,  1,  3,  2,  1,  1,  3,  1,  3,  3,  2,\n",
       "        3,  1,  2,  2,  1,  2,  2,  1,  3,  2,  1,  3,  2,  2,  0,  0,  0,\n",
       "        4,  1,  1,  0,  0,  2,  4,  3,  1,  2,  2,  1,  1,  1,  0,  1,  0,\n",
       "        0,  1,  6,  1,  3,  2,  1,  2,  1,  1,  0,  0,  0,  2,  1,  1,  3,\n",
       "        1,  1,  0,  2,  0,  0,  0,  2,  0,  0,  0,  0,  4,  0,  0,  2,  0,\n",
       "        2,  2,  0,  0,  0,  0,  2,  1,  0,  0,  1,  1,  0,  3,  0,  0,  1,\n",
       "        0,  2,  0,  0,  1,  1,  0,  0,  1,  0,  1,  0,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  1,  0,  0,  0,  0,  0,\n",
       "        2,  1,  1,  0,  0,  0,  0,  5,  1,  0,  0,  0,  0,  1,  0,  2,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  1,  0,  1,  0,  0,\n",
       "        0,  2,  0,  1,  0,  0,  2,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,\n",
       "        6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  3,  0,\n",
       "        0,  0,  1,  0,  2,  0,  1,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,\n",
       "        0,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
       "        0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  1,  6,  0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  5,  2,\n",
       "        0,  0,  0,  0,  0,  0,  1,  0,  2,  0,  0,  0,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  2,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lgb.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_importances2 = pd.DataFrame(model_lgb.feature_importances_, index=X_train.columns, columns=[\"importancia\"]).\\\n",
    "        sort_values(by=\"importancia\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    492\n",
       "1     72\n",
       "2     51\n",
       "3     46\n",
       "4     39\n",
       "Name: importancia, dtype: int64"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feat_importances2[\"importancia\"].value_counts().nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 308)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_fi = df_feat_importances2.index[:308].tolist()\n",
    "X = X.filter(items=list_fi)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.5,\n",
       "               importance_type='split', learning_rate=0.2, max_depth=7,\n",
       "               min_child_samples=10, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=250, n_jobs=1, num_leaves=7, objective='binary',\n",
       "               random_state=None, reg_alpha=0.0, reg_lambda=1, silent=True,\n",
       "               subsample=1, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#usando 308 features\n",
    "model_lgb = lgb.LGBMClassifier(num_leaves=7, objective=\"binary\", colsample_bytree=0.5, subsample=1, max_depth=7, n_jobs=1,\n",
    "                              n_estimators=250, min_child_samples=10, reg_lambda=1, learning_rate=0.2)\n",
    "model_lgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.790718\n"
     ]
    }
   ],
   "source": [
    "y_test_hat = model_lgb.predict(X_test)\n",
    "print(\"Accuracy score: %f\" % (accuracy_score(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
