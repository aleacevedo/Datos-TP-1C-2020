{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from bert import bert_tokenization\n",
    "from tensorflow.keras.models import Model       # Keras is the new high level API for TensorFlow\n",
    "import math\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "FullTokenizer = bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
    "max_seq_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/train_super_featured.csv')\n",
    "test_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_super_featured.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['keyword_original'].fillna('no_keyword', inplace=True)\n",
    "test_data['keyword_original'].fillna('no_keyword', inplace=True)\n",
    "train_data['location_original'].fillna('no_location', inplace=True)\n",
    "test_data['location_original'].fillna('no_location', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_original'].fillna('', inplace=True)\n",
    "train_data['clean_text'].fillna('', inplace=True)\n",
    "train_data['super_clean_text'].fillna('', inplace=True)\n",
    "train_data['kaggle_text'].fillna('', inplace=True)\n",
    "train_data['semi_cleaned_text'].fillna('', inplace=True)\n",
    "test_data['text_original'].fillna('', inplace=True)\n",
    "test_data['clean_text'].fillna('', inplace=True)\n",
    "test_data['super_clean_text'].fillna('', inplace=True)\n",
    "test_data['kaggle_text'].fillna('', inplace=True)\n",
    "test_data['semi_cleaned_text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_masks(tokens, max_seq_length):\n",
    "    \"\"\"Mask for padding\"\"\"\n",
    "    if len(tokens)>max_seq_length:\n",
    "        #Cutting down the excess length\n",
    "        tokens = tokens[0:max_seq_length]\n",
    "        return [1]*len(tokens)\n",
    "    else :\n",
    "      return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_segments(tokens, max_seq_length):\n",
    "    if len(tokens)>max_seq_length:\n",
    "      #Cutting down the excess length\n",
    "      tokens = tokens[:max_seq_length]\n",
    "      segments = []\n",
    "      current_segment_id = 0\n",
    "      for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "          current_segment_id = 1\n",
    "      return segments\n",
    "    else:\n",
    "      segments = []\n",
    "      current_segment_id = 0\n",
    "      for token in tokens:\n",
    "        segments.append(current_segment_id)\n",
    "        if token == \"[SEP]\":\n",
    "          current_segment_id = 1\n",
    "      return segments + [0] * (max_seq_length - len(tokens))\n",
    "\n",
    "def get_ids(tokens, tokenizer, max_seq_length):    \n",
    "    if len(tokens)>max_seq_length:\n",
    "      tokens = tokens[:max_seq_length]\n",
    "      token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      return token_ids\n",
    "    else:\n",
    "      token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "      input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
    "      return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep(s, get = 'id'):\n",
    "  stokens = tokenizer.tokenize(s)\n",
    "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
    "  if get == 'id':\n",
    "    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n",
    "    return input_ids\n",
    "  elif get == 'mask':\n",
    "    input_masks = get_masks(stokens, max_seq_length)\n",
    "    return input_masks\n",
    "  else:\n",
    "    input_segments = get_segments(stokens, max_seq_length)\n",
    "    return input_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')\n",
    "input_mask = Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')\n",
    "segment_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')    \n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])   \n",
    "clf_output = sequence_output[:, 0, :]\n",
    "out = Dense(1, activation='sigmoid')(clf_output)\n",
    "\n",
    "model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "optimizer = SGD(learning_rate=0.001, momentum=0.8)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tweets):\n",
    "    all_ids = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    for tweet in tweets:\n",
    "        s1 = tweet\n",
    "        stokens1 = tokenizer.tokenize(s1)\n",
    "        stokens1 = [\"[CLS]\"] + stokens1 + [\"[SEP]\"]\n",
    "\n",
    "        input_ids = get_ids(stokens1, tokenizer, max_seq_length)\n",
    "        input_masks = get_masks(stokens1, max_seq_length)\n",
    "        input_segments = get_segments(stokens1, max_seq_length)\n",
    "        \n",
    "        all_ids.append(input_ids)\n",
    "        all_masks.append(input_masks)\n",
    "        all_segments.append(input_segments)\n",
    "    \n",
    "    return np.array(all_ids), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_encoded = encode(train_data['super_clean_text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='loss', min_delta=0.001, patience=8, verbose=1)\n",
    "callbacks_list = [early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.6034 - accuracy: 0.6832 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 2728s 23s/step - loss: 0.6034 - accuracy: 0.6832\n",
      "Epoch 2/5\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4991 - accuracy: 0.7709 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 2694s 23s/step - loss: 0.4991 - accuracy: 0.7709\n",
      "Epoch 3/5\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.7904 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 2698s 23s/step - loss: 0.4613 - accuracy: 0.7904\n",
      "Epoch 4/5\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4397 - accuracy: 0.8020 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 2696s 23s/step - loss: 0.4397 - accuracy: 0.8020\n",
      "Epoch 5/5\n",
      "119/119 [==============================] - ETA: 0s - loss: 0.4234 - accuracy: 0.8114 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "119/119 [==============================] - 2706s 23s/step - loss: 0.4234 - accuracy: 0.8114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1444f4358>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tweets_encoded, train_data['target_relabeled'], callbacks=callbacks_list, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encode(test_data['super_clean_text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.87771785],\n",
       "       [0.87212855],\n",
       "       [0.8944807 ],\n",
       "       ...,\n",
       "       [0.915142  ],\n",
       "       [0.85924244],\n",
       "       [0.94016445]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liked = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_with_targets.csv', dtype={'id': np.int16, 'target': np.int8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7977321483297579"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.round(y_pred).astype('int')\n",
    "np.mean(y_pred.flatten() == df_liked.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
