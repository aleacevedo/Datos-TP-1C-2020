{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>target_label</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>...</th>\n",
       "      <th>super_stop_word_count</th>\n",
       "      <th>super_url_count</th>\n",
       "      <th>super_mean_word_length</th>\n",
       "      <th>super_char_count</th>\n",
       "      <th>super_punctuation_count</th>\n",
       "      <th>super_hashtag_count</th>\n",
       "      <th>super_mention_count</th>\n",
       "      <th>super_clean_text</th>\n",
       "      <th>kaggle_text</th>\n",
       "      <th>target_relabeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.38</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>Our Deeds are the Reason of this  # earthquake...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.57</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>Forest fire near La Ronge Sask .  Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>133</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>All residents asked to  ' shelter in place '  ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.12</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.73</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>Just got sent this photo from Ruby  # Alaska a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_original keyword_original location_original  \\\n",
       "0            1              NaN               NaN   \n",
       "1            4              NaN               NaN   \n",
       "2            5              NaN               NaN   \n",
       "3            6              NaN               NaN   \n",
       "4            7              NaN               NaN   \n",
       "\n",
       "                                       text_original  target_label  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...             1   \n",
       "1             Forest fire near La Ronge Sask. Canada             1   \n",
       "2  All residents asked to 'shelter in place' are ...             1   \n",
       "3  13,000 people receive #wildfires evacuation or...             1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...             1   \n",
       "\n",
       "   special_chars_count             hashtags labels  hashtags_count  \\\n",
       "0                    1         #earthquake     NaN               1   \n",
       "1                    1                  NaN    NaN               0   \n",
       "2                    3                  NaN    NaN               0   \n",
       "3                    2          #wildfires     NaN               1   \n",
       "4                    2  #alaska #wildfires     NaN               2   \n",
       "\n",
       "   labels_count  ...  super_stop_word_count  super_url_count  \\\n",
       "0             0  ...                      6                0   \n",
       "1             0  ...                      0                0   \n",
       "2             0  ...                     11                0   \n",
       "3             0  ...                      1                0   \n",
       "4             0  ...                      6                0   \n",
       "\n",
       "  super_mean_word_length super_char_count  super_punctuation_count  \\\n",
       "0                   4.38               69                        1   \n",
       "1                   4.57               38                        1   \n",
       "2                   5.09              133                        3   \n",
       "3                   7.12               65                        2   \n",
       "4                   4.73               87                        2   \n",
       "\n",
       "   super_hashtag_count  super_mention_count  \\\n",
       "0                    1                    0   \n",
       "1                    0                    0   \n",
       "2                    0                    0   \n",
       "3                    1                    0   \n",
       "4                    2                    0   \n",
       "\n",
       "                                    super_clean_text  \\\n",
       "0       deeds reason earthquake may allah forgive us   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  people receive wildfires evacuation orders cal...   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                         kaggle_text  target_relabeled  \n",
       "0  Our Deeds are the Reason of this  # earthquake...                 1  \n",
       "1           Forest fire near La Ronge Sask .  Canada                 1  \n",
       "2  All residents asked to  ' shelter in place '  ...                 1  \n",
       "3  13,000 people receive  # wildfires evacuation ...                 1  \n",
       "4  Just got sent this photo from Ruby  # Alaska a...                 1  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/train_super_featured.csv')\n",
    "test_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_super_featured.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_original'].fillna('', inplace=True)\n",
    "train_data['clean_text'].fillna('', inplace=True)\n",
    "train_data['super_clean_text'].fillna('', inplace=True)\n",
    "train_data['kaggle_text'].fillna('', inplace=True)\n",
    "train_data['semi_cleaned_text'].fillna('', inplace=True)\n",
    "test_data['text_original'].fillna('', inplace=True)\n",
    "test_data['clean_text'].fillna('', inplace=True)\n",
    "test_data['super_clean_text'].fillna('', inplace=True)\n",
    "test_data['kaggle_text'].fillna('', inplace=True)\n",
    "test_data['semi_cleaned_text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data['target_relabeled'].values.tolist()\n",
    "data = train_data['super_clean_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = \\\n",
    "    train_test_split(data,target, test_size=0.2)  # random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1477d7400>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()             #Tokenizer(num_words=5000) => 5000 words of the highest frequency\n",
    "tokenizer.fit_on_texts(data)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenizer) : 14346\n"
     ]
    }
   ],
   "source": [
    "print(\"len(tokenizer) :\",len(list(tokenizer.word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9729410219361618"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_tokens : [883, 884, 49, 245, 800, 678, 652, 64, 279, 385, 3839, 652, 1005, 279, 46, 49]\n",
      "x_train_pad : [ 883  884   49  245  800  678  652   64  279  385 3839  652 1005  279\n",
      "   46   49]\n"
     ]
    }
   ],
   "source": [
    "#Zero is added before the values given in the padding operation.\n",
    "\n",
    "print(\"x_train_tokens :\",x_train_tokens[0])\n",
    "print(\"x_train_pad :\",x_train_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_pad.shape : (6090, 16)\n",
      "x_train_pad.shape : (1523, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train_pad.shape :\",x_train_pad.shape)\n",
    "print(\"x_train_pad.shape :\",x_test_pad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calgary area tornado warnings end thunderstorms move eastward cbcca cbcca calgary area tornado warnings end u'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[800]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[334, 222, 419, 1610, 194, 1542, 648, 6128, 4188, 4188, 334, 222, 419, 1610, 194, 1]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tokens[800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'calgary area tornado warnings end thunderstorms move eastward cbcca cbcca calgary area tornado warnings end u'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(x_train_tokens[800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open('../../../DataSets/TP2/glove.840B.300d.txt', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(X):\n",
    "    \n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 51.90% of vocabulary and 81.71% of text in Training Set text_original\n",
      "GloVe Embeddings cover 57.12% of vocabulary and 81.00% of text in Test Set text_original\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['text_original'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['text_original'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set text_original'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set text_original'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 85.76% of vocabulary and 95.89% of text in Training Set clean_text\n",
      "GloVe Embeddings cover 88.06% of vocabulary and 95.66% of text in Test Set clean_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['clean_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['clean_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set clean_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set clean_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 67.95% of vocabulary and 94.84% of text in Training Set kaggle_text\n",
      "GloVe Embeddings cover 75.66% of vocabulary and 94.99% of text in Test Set kaggle_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['kaggle_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['kaggle_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set kaggle_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set kaggle_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 86.11% of vocabulary and 96.09% of text in Training Set super_clean_text\n",
      "GloVe Embeddings cover 89.24% of vocabulary and 96.12% of text in Test Set super_clean_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['super_clean_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['super_clean_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set super_clean_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set super_clean_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test[0] : im going say new reddit policy changes think r conspiracy quarantined thats bad truth u\n",
      "x_test_pad[0] : [   0    0    0    0    0    0    0  796  248 1944  742 3239 2207  284\n",
      " 1319 6034]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_test[0] :\",x_test[40])\n",
    "print(\"x_test_pad[0] :\",x_test_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(list(tokenizer.word_index)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.random.uniform(-1, 1, (num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14347, 300)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.44820014e-02,  5.82930028e-01, -7.82339990e-01, -4.49389994e-01,\n",
       "        1.93560004e-01,  1.68380007e-01,  2.84999996e-01, -4.01809990e-01,\n",
       "       -7.54270032e-02,  1.16149998e+00, -5.73189974e-01,  2.72560000e-01,\n",
       "       -1.46229997e-01, -4.52829987e-01,  7.65789986e-01,  1.09020002e-01,\n",
       "       -2.05180004e-01,  1.00919998e+00, -3.18250000e-01,  4.14839983e-02,\n",
       "       -5.20990007e-02,  1.25279993e-01,  6.20520003e-02, -4.52399999e-01,\n",
       "       -2.21540004e-01, -2.54539996e-01, -3.68680000e-01, -3.57010007e-01,\n",
       "        8.97620022e-01, -2.88789988e-01, -2.53369987e-01,  1.88620001e-01,\n",
       "        1.54560000e-01,  1.91569999e-01,  1.16980001e-01,  1.17820002e-01,\n",
       "       -2.23859996e-01,  1.61149994e-01, -1.17160000e-01, -1.72269996e-02,\n",
       "       -1.79370001e-01, -5.06579995e-01, -4.70730007e-01,  2.95810014e-01,\n",
       "       -2.33830005e-01,  5.29030025e-01,  2.74870008e-01, -8.10769975e-01,\n",
       "       -2.54729986e-01, -2.86720008e-01, -1.02229998e-01, -1.10290004e-02,\n",
       "       -1.66869998e-01, -3.60289991e-01,  1.67089999e-01,  5.27939975e-01,\n",
       "       -5.41329980e-01, -6.03429973e-01,  5.53969979e-01, -8.68860036e-02,\n",
       "        5.12549980e-03, -4.57700014e-01,  9.95360035e-03, -4.85929996e-01,\n",
       "       -1.92860007e-01, -4.68959987e-01, -8.04660022e-02, -8.00009966e-02,\n",
       "        1.54909998e-01,  2.67749995e-01,  1.83739997e-02,  1.99059993e-01,\n",
       "        1.29999995e-01, -5.16470015e-01,  4.24409986e-01,  2.26489995e-02,\n",
       "        3.41129988e-01,  6.21259987e-01, -5.75820029e-01,  9.49620008e-01,\n",
       "        2.59860009e-01,  5.13909996e-01,  3.08550000e-01,  5.39899990e-02,\n",
       "        3.28729987e-01, -1.74350000e-03,  1.18830001e+00, -8.23719978e-01,\n",
       "       -5.21549992e-02,  1.22429997e-01, -7.27820024e-02, -4.53130007e-01,\n",
       "       -4.29520011e-01, -5.24680018e-01, -4.57050018e-02, -2.61940002e-01,\n",
       "        1.56320006e-01, -3.80479991e-01, -8.25309992e-01,  3.02660000e-02,\n",
       "        2.09500000e-01, -4.06679988e-01,  9.91759971e-02,  7.52729997e-02,\n",
       "        9.24669981e-01, -2.88150012e-01,  3.50960009e-02, -5.71170002e-02,\n",
       "        6.76079988e-02, -8.62709999e-01, -3.32109988e-01, -1.04079998e+00,\n",
       "       -3.50609988e-01, -4.77759987e-01,  5.68349995e-02,  3.02900001e-02,\n",
       "        5.90439998e-02, -4.05110002e-01, -7.08609998e-01, -1.99539997e-02,\n",
       "        3.47149998e-01, -7.36000016e-02,  7.54719973e-01, -1.40740007e-01,\n",
       "        7.77170002e-01,  4.56050009e-01, -2.92140007e-01, -3.68059993e-01,\n",
       "        2.15310007e-01, -1.59760006e-02,  1.02329999e-02, -1.86410006e-02,\n",
       "       -4.19470012e-01, -1.36700004e-01,  1.14940000e+00, -4.08230007e-01,\n",
       "       -3.49640012e-01, -3.95799994e-01, -3.12169999e-01, -2.95520008e-01,\n",
       "       -3.06180000e+00,  1.84450001e-02,  1.60559997e-01, -9.87590030e-02,\n",
       "       -4.61149991e-01,  1.40609995e-01, -5.34879982e-01,  5.62870026e-01,\n",
       "        7.53690004e-02, -5.60850024e-01, -6.20100021e-01,  1.29189998e-01,\n",
       "        7.28640035e-02, -2.83309996e-01, -1.09930001e-02,  5.26589990e-01,\n",
       "       -2.79509991e-01, -2.56249994e-01, -4.87129986e-01, -2.25799996e-02,\n",
       "        6.48670018e-01, -1.56890005e-01, -1.81539997e-01, -1.14650000e-02,\n",
       "       -3.62910002e-01,  1.34690002e-01, -2.78430015e-01,  1.53109998e-01,\n",
       "       -2.12469995e-02,  7.11370036e-02, -6.81119978e-01, -3.22389990e-01,\n",
       "       -4.08890009e-01, -3.81669998e-01,  1.53200001e-01, -4.72119987e-01,\n",
       "       -9.32449996e-01,  5.63480020e-01, -1.18770003e-01,  5.30420005e-01,\n",
       "        2.57450007e-02,  1.74209997e-01, -1.01040006e+00, -4.30009998e-02,\n",
       "       -7.92929977e-02,  2.57620007e-01, -1.48929998e-01, -5.49339987e-02,\n",
       "        3.80070001e-01,  4.51940000e-02,  4.35820013e-01,  3.41789983e-02,\n",
       "       -8.36420000e-01, -2.50340015e-01,  3.56370002e-01,  1.05669999e+00,\n",
       "        2.05909997e-01, -4.75149989e-01, -3.17520015e-02, -5.46480000e-01,\n",
       "       -7.76259974e-02, -4.70779985e-01,  1.84760004e-01, -4.42509986e-02,\n",
       "        2.61240005e-01, -9.27259997e-02, -2.45769992e-02,  1.57490000e-01,\n",
       "       -2.76740007e-02, -3.70790005e-01, -4.40050006e-01,  3.83810014e-01,\n",
       "        3.67100000e-01,  4.84719984e-02,  1.92570001e-01,  1.53900003e-02,\n",
       "       -1.12319998e-01, -4.02900010e-01, -2.95870006e-01,  2.13880002e-01,\n",
       "        1.63710006e-02, -1.88270003e-01,  2.01660007e-01, -5.58939993e-01,\n",
       "       -4.56790000e-01, -9.66330022e-02,  2.47040004e-01,  1.73429996e-02,\n",
       "        9.86230001e-03, -6.35819972e-01, -4.20989990e-01,  2.26439998e-01,\n",
       "        3.31330001e-01, -3.28640014e-01,  2.33720005e-01, -1.79800004e-01,\n",
       "       -4.05970007e-01,  7.08020031e-02, -6.48220003e-01,  3.35579991e-01,\n",
       "        1.22570001e-01,  2.88500011e-01,  1.17540002e-01,  7.11390018e-01,\n",
       "       -1.32620007e-01, -1.74449995e-01,  4.14629996e-01,  3.44619989e-01,\n",
       "        4.89069998e-01,  4.43399996e-01,  5.26459992e-01,  1.99010000e-01,\n",
       "       -1.76479995e-01,  6.86360002e-02, -5.15739992e-02, -3.76780003e-01,\n",
       "        1.59580007e-01,  2.20740005e-01, -1.71210006e-01, -3.50699991e-01,\n",
       "        2.33620003e-01,  3.33909988e-01,  4.49409992e-01,  2.37629996e-04,\n",
       "       -3.70739996e-01, -1.10450006e+00, -2.30299994e-01,  4.09130007e-02,\n",
       "        9.82690006e-02,  4.89740014e-01,  5.87290004e-02, -2.69120008e-01,\n",
       "       -5.27369976e-01,  4.66289997e-01,  2.73090005e-01,  3.53269994e-01,\n",
       "       -5.28240018e-02, -1.18230000e-01,  1.16910003e-01, -1.42550007e-01,\n",
       "        6.37579978e-01, -8.04539993e-02, -2.29750007e-01, -1.34179994e-01,\n",
       "        1.41570002e-01, -2.08460003e-01,  1.75919995e-01,  2.35420004e-01,\n",
       "        3.08719993e-01,  4.22890007e-01,  2.06060000e-02, -3.81889999e-01,\n",
       "       -6.80069983e-01, -8.86479974e-01,  7.63679981e-01,  2.95370013e-01,\n",
       "        3.37210000e-01, -2.49840006e-01, -9.69529971e-02,  6.66920006e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = max_tokens\n",
    "vocabulary_size = num_words\n",
    "embedding_dim = embedding_size\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = np.array([])\n",
    "y_test2 = np.array([])\n",
    "for i in y_train:\n",
    "    y_train2 = np.append(y_train2, i)\n",
    "for i in y_test:\n",
    "    y_test2 =np.append(y_test2, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training CNN ...\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 16, 300)           4304100   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 16, 32)            67232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 8, 32)             7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,379,654\n",
      "Trainable params: 4,379,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "11/11 - 1s - loss: 0.9471 - accuracy: 0.6262 - val_loss: 0.8554 - val_accuracy: 0.7438\n",
      "Epoch 2/5\n",
      "11/11 - 1s - loss: 0.7781 - accuracy: 0.7727 - val_loss: 0.7694 - val_accuracy: 0.7652\n",
      "Epoch 3/5\n",
      "11/11 - 1s - loss: 0.6762 - accuracy: 0.8194 - val_loss: 0.7293 - val_accuracy: 0.7652\n",
      "Epoch 4/5\n",
      "11/11 - 1s - loss: 0.5973 - accuracy: 0.8519 - val_loss: 0.7077 - val_accuracy: 0.7865\n",
      "Epoch 5/5\n",
      "11/11 - 1s - loss: 0.5270 - accuracy: 0.8765 - val_loss: 0.6864 - val_accuracy: 0.7783\n"
     ]
    }
   ],
   "source": [
    "#CNN architecture\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#Training params\n",
    "batch_size = 512 \n",
    "num_epochs = 5\n",
    "\n",
    "#Model parameters\n",
    "num_filters = 32  # gÃ¶rÃ¼ntÃ¼nÃ¼n boyutu mesela 512*512\n",
    "embed_dim = embedding_size \n",
    "weight_decay = 1e-2\n",
    "\n",
    "print(\"training CNN ...\")\n",
    "model = Sequential()\n",
    "\n",
    "#Model add word2vec embedding\n",
    "\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    weights= [embedding_matrix],\n",
    "                    input_length=max_tokens,        \n",
    "                    trainable=True,              #the layer is trained\n",
    "                    name='embedding_layer'))\n",
    "model.add(Conv1D(num_filters, 7, activation='tanh', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(num_filters, 7, activation='tanh', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
    "\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "#define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=8, verbose=1)\n",
    "callbacks_list = [early_stopping]\n",
    "\n",
    "\n",
    "hist = model.fit(x_train_pad, y_train2, batch_size=batch_size, epochs=num_epochs, callbacks=callbacks_list, validation_split=0.1, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8273145108338805"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model.predict(x_test_pad)\n",
    "predicted =np.argmax(predicted, axis=1) \n",
    "np.mean(predicted == y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final_test_tokens = tokenizer.texts_to_sequences(test_data['super_clean_text'])\n",
    "x_final_test_pad = pad_sequences(x_final_test_tokens, maxlen=max_tokens)\n",
    "predicted = model.predict(x_final_test_pad)\n",
    "predicted =np.argmax(predicted, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['target'] = predicted\n",
    "test_data[['id_original', 'target']].rename(columns={'id_original': 'id'}).to_csv('~/Documents/Datos/DataSets/TP2/res_TF_GloVe.2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
