{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>target_label</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>68</td>\n",
       "      <td>4.31</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>37</td>\n",
       "      <td>4.43</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>130</td>\n",
       "      <td>4.95</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>56</td>\n",
       "      <td>7.14</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>85</td>\n",
       "      <td>4.38</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_original keyword_original location_original  \\\n",
       "0            1              NaN               NaN   \n",
       "1            4              NaN               NaN   \n",
       "2            5              NaN               NaN   \n",
       "3            6              NaN               NaN   \n",
       "4            7              NaN               NaN   \n",
       "\n",
       "                                       text_original  target_label  \\\n",
       "0  our deeds are the reason of this earthquake ma...             1   \n",
       "1              forest fire near la ronge sask canada             1   \n",
       "2  all residents asked to shelter in place are be...             1   \n",
       "3  people receive wildfires evacuation orders in ...             1   \n",
       "4  just got sent this photo from ruby alaska as s...             1   \n",
       "\n",
       "   special_chars_count             hashtags labels  hashtags_count  \\\n",
       "0                    1         #earthquake     NaN               1   \n",
       "1                    1                  NaN    NaN               0   \n",
       "2                    3                  NaN    NaN               0   \n",
       "3                    2          #wildfires     NaN               1   \n",
       "4                    2  #alaska #wildfires     NaN               2   \n",
       "\n",
       "   labels_count  num_chars_count  links_count  \\\n",
       "0             0                0            0   \n",
       "1             0                0            0   \n",
       "2             0                0            0   \n",
       "3             0                5            0   \n",
       "4             0                0            0   \n",
       "\n",
       "                                          clean_text  text_length  \\\n",
       "0       deeds reason earthquake may allah forgive us           68   \n",
       "1              forest fire near la ronge sask canada           37   \n",
       "2  residents asked shelter place notified officer...          130   \n",
       "3  people receive wildfires evacuation orders cal...           56   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...           85   \n",
       "\n",
       "      mean_word_length  vowels_count  short_words_count  stopwords_count  \\\n",
       "0                 4.31            25                  7                6   \n",
       "1                 4.43            13                  1                0   \n",
       "2                 4.95            45                  9               11   \n",
       "3                 7.14            24                  1                1   \n",
       "4                 4.38            25                  3                7   \n",
       "\n",
       "   words_count  \n",
       "0           13  \n",
       "1            7  \n",
       "2           22  \n",
       "3            7  \n",
       "4           16  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twt_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/train_featured.csv')\n",
    "twt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(\"(?P<url>https?://[^\\s]+)\")\n",
    "def remove_link(twt):\n",
    "    return pattern.sub(\"r \", twt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "twt_data['text'] = twt_data['text'].map(remove_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(twt_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 14190)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(stop_words='english')\n",
    "X_train_counts = count_vect.fit_transform(twt_data['clean_text'])\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 14190)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aaaand',\n",
       " 'aaalll',\n",
       " 'aaarrrgghhh',\n",
       " 'aaemiddleaged',\n",
       " 'aal',\n",
       " 'aan',\n",
       " 'aannnd',\n",
       " 'aar',\n",
       " 'aashiqui',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abbandoned',\n",
       " 'abbott',\n",
       " 'abbruchsimulator',\n",
       " 'abbswinston',\n",
       " 'abc',\n",
       " 'abceyewitness',\n",
       " 'abcnews',\n",
       " 'abcs',\n",
       " 'abe',\n",
       " 'aberdeen',\n",
       " 'aberystwythshrewsbury',\n",
       " 'abes',\n",
       " 'abha',\n",
       " 'abia',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ableg',\n",
       " 'aboard',\n",
       " 'abomb',\n",
       " 'abombed',\n",
       " 'abomination',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abouts',\n",
       " 'abq',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'abstorm',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'ac',\n",
       " 'academia',\n",
       " 'acc',\n",
       " 'accept',\n",
       " 'accepte',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accident',\n",
       " 'accidentally',\n",
       " 'accidentalprophecy',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'accompanying',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'account',\n",
       " 'accountable',\n",
       " 'accounts',\n",
       " 'accuracy',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accustomed',\n",
       " 'acdelco',\n",
       " 'ace',\n",
       " 'acenewsdesk',\n",
       " 'acesse',\n",
       " 'achedin',\n",
       " 'achieve',\n",
       " 'achievement',\n",
       " 'achieving',\n",
       " 'achimota',\n",
       " 'aching',\n",
       " 'acid',\n",
       " 'acids',\n",
       " 'acne',\n",
       " 'acoustic',\n",
       " 'acquiesce',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquisitions',\n",
       " 'acres',\n",
       " 'acronym',\n",
       " 'acrylic',\n",
       " 'act',\n",
       " 'actavis',\n",
       " 'acted',\n",
       " 'actin',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actionmoviestaughtus',\n",
       " 'actions',\n",
       " 'activate',\n",
       " 'activated',\n",
       " 'activates',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activision',\n",
       " 'activist',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actress',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'acura',\n",
       " 'acute',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'adamantly',\n",
       " 'adani',\n",
       " 'adaptation',\n",
       " 'add',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addiction',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'address',\n",
       " 'addresses',\n",
       " 'addtexastonextdtour',\n",
       " 'adelaide',\n",
       " 'adhd',\n",
       " 'adidas',\n",
       " 'adiossuperbacterias',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'adjuster',\n",
       " 'admin',\n",
       " 'administration',\n",
       " 'administrative',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'adopt',\n",
       " 'adoption',\n",
       " 'adoptive',\n",
       " 'adorable',\n",
       " 'ads',\n",
       " 'adsit',\n",
       " 'adult',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advances',\n",
       " 'advantages',\n",
       " 'adventures',\n",
       " 'adverse',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advice',\n",
       " 'advised',\n",
       " 'advisory',\n",
       " 'ae',\n",
       " 'aedemolition',\n",
       " 'aefedex',\n",
       " 'aeg',\n",
       " 'aei',\n",
       " 'aem',\n",
       " 'aemgnafricaa',\n",
       " 'aeroplane',\n",
       " 'aerospace',\n",
       " 'aesop',\n",
       " 'aesthetic',\n",
       " 'af',\n",
       " 'afc',\n",
       " 'affair',\n",
       " 'affect',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'affiliate',\n",
       " 'affiliation',\n",
       " 'affleck',\n",
       " 'affliction',\n",
       " 'afghan',\n",
       " 'afghanistan',\n",
       " 'afghetc',\n",
       " 'afloat',\n",
       " 'afp',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africanbaze',\n",
       " 'africans',\n",
       " 'africansinsf',\n",
       " 'africas',\n",
       " 'afrikaan',\n",
       " 'afrin',\n",
       " 'afte',\n",
       " 'afterhaiyan',\n",
       " 'afterhours',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'aftershock',\n",
       " 'aftershocks',\n",
       " 'afycso',\n",
       " 'ag',\n",
       " 'agalloch',\n",
       " 'agdq',\n",
       " 'age',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggarwal',\n",
       " 'aggressif',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agnivesh',\n",
       " 'agnus',\n",
       " 'ago',\n",
       " 'agochicago',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreed',\n",
       " 'agreements',\n",
       " 'agrees',\n",
       " 'agreeshe',\n",
       " 'aguero',\n",
       " 'agw',\n",
       " 'ah',\n",
       " 'ahahahga',\n",
       " 'ahamedis',\n",
       " 'ahead',\n",
       " 'ahh',\n",
       " 'ahhh',\n",
       " 'ahhhhh',\n",
       " 'ahmazing',\n",
       " 'ahrar',\n",
       " 'ahuh',\n",
       " 'ai',\n",
       " 'aias',\n",
       " 'aid',\n",
       " 'aidan',\n",
       " 'aids',\n",
       " 'aiii',\n",
       " 'aim',\n",
       " 'aimlessly',\n",
       " 'aint',\n",
       " 'aintsheperty',\n",
       " 'air',\n",
       " 'airasia',\n",
       " 'aircraft',\n",
       " 'airhead',\n",
       " 'airhorns',\n",
       " 'airing',\n",
       " 'airlift',\n",
       " 'airlines',\n",
       " 'airplane',\n",
       " 'airplaneae',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airports',\n",
       " 'airstrikes',\n",
       " 'airwaves',\n",
       " 'aisle',\n",
       " 'ajw',\n",
       " 'ak',\n",
       " 'aka',\n",
       " 'akame',\n",
       " 'akilah',\n",
       " 'akito',\n",
       " 'akrams',\n",
       " 'aks',\n",
       " 'akwa',\n",
       " 'akx',\n",
       " 'akxbskdn',\n",
       " 'al',\n",
       " 'alabama',\n",
       " 'alabamaquake',\n",
       " 'aladdin',\n",
       " 'alameda',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alarmingly',\n",
       " 'alarms',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'alaskan',\n",
       " 'alaskas',\n",
       " 'alaskaseafood',\n",
       " 'alba',\n",
       " 'albany',\n",
       " 'albeit',\n",
       " 'alberta',\n",
       " 'albertans',\n",
       " 'albertas',\n",
       " 'albertsons',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alchemist',\n",
       " 'alcohol',\n",
       " 'alcoholism',\n",
       " 'aldridge',\n",
       " 'alec',\n",
       " 'alert',\n",
       " 'alerts',\n",
       " 'alex',\n",
       " 'alexandrian',\n",
       " 'alexis',\n",
       " 'algae',\n",
       " 'algeria',\n",
       " 'alhaji',\n",
       " 'ali',\n",
       " 'alice',\n",
       " 'alien',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'alil',\n",
       " 'alisonannyoung',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allay',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allergic',\n",
       " 'alley',\n",
       " 'alliance',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'allin',\n",
       " 'alll',\n",
       " 'alllivesmatter',\n",
       " 'allocating',\n",
       " 'alloosh',\n",
       " 'allotment',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alloy',\n",
       " 'allpro',\n",
       " 'allthekidneybeansandsorbetmisha',\n",
       " 'allthenews',\n",
       " 'alltime',\n",
       " 'ally',\n",
       " 'almighty',\n",
       " 'alois',\n",
       " 'alot',\n",
       " 'alpha',\n",
       " 'alphen',\n",
       " 'alps',\n",
       " 'alrasyiditurasya',\n",
       " 'alright',\n",
       " 'alrighty',\n",
       " 'alska',\n",
       " 'alsowhat',\n",
       " 'alt',\n",
       " 'altamonte',\n",
       " 'alternate',\n",
       " 'alternative',\n",
       " 'alternatives',\n",
       " 'alton',\n",
       " 'aluminum',\n",
       " 'alves',\n",
       " 'alwsl',\n",
       " 'alwx',\n",
       " 'ama',\n",
       " 'amageddon',\n",
       " 'amalie',\n",
       " 'amateur',\n",
       " 'amazed',\n",
       " 'amazin',\n",
       " 'amazing',\n",
       " 'amazingness',\n",
       " 'amazon',\n",
       " 'amazondeals',\n",
       " 'amazons',\n",
       " 'amber',\n",
       " 'ambition',\n",
       " 'ambleside',\n",
       " 'ambulance',\n",
       " 'ambulances',\n",
       " 'ambulancewe',\n",
       " 'amcx',\n",
       " 'amdollela',\n",
       " 'amen',\n",
       " 'amends',\n",
       " 'ameribag',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'ames',\n",
       " 'amicos',\n",
       " 'amicospizzato',\n",
       " 'amid',\n",
       " 'amiibos',\n",
       " 'amino',\n",
       " 'amirite',\n",
       " 'amman',\n",
       " 'amo',\n",
       " 'amp',\n",
       " 'amplifier',\n",
       " 'amreading',\n",
       " 'amritsar',\n",
       " 'amsal',\n",
       " 'amssummer',\n",
       " 'amsterdam',\n",
       " 'ana',\n",
       " 'anakin',\n",
       " 'analog',\n",
       " 'analysis',\n",
       " 'anarchy',\n",
       " 'anatomy',\n",
       " 'anchor',\n",
       " 'anchorage',\n",
       " 'anchors',\n",
       " 'ancient',\n",
       " 'ancop',\n",
       " 'andaechina',\n",
       " 'anders',\n",
       " 'anderson',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andrew',\n",
       " 'android',\n",
       " 'androidgames',\n",
       " 'ands',\n",
       " 'andy',\n",
       " 'anew',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelina',\n",
       " 'angelriveralib',\n",
       " 'angels',\n",
       " 'anger',\n",
       " 'angers',\n",
       " 'angioplasty',\n",
       " 'angry',\n",
       " 'ani',\n",
       " 'animal',\n",
       " 'animalrescue',\n",
       " 'animals',\n",
       " 'animations',\n",
       " 'animatronics',\n",
       " 'anime',\n",
       " 'aniston',\n",
       " 'anjem',\n",
       " 'ankle',\n",
       " 'ankles',\n",
       " 'anna',\n",
       " 'annddd',\n",
       " 'annihilate',\n",
       " 'annihilated',\n",
       " 'annihilating',\n",
       " 'annihilation',\n",
       " 'anniversary',\n",
       " 'annonymous',\n",
       " 'annoucement',\n",
       " 'announce',\n",
       " 'announced',\n",
       " 'announcement',\n",
       " 'announces',\n",
       " 'annoyed',\n",
       " 'annoying',\n",
       " 'annual',\n",
       " 'anonymous',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'answers',\n",
       " 'ant',\n",
       " 'ante',\n",
       " 'anthelmintic',\n",
       " 'anthology',\n",
       " 'anthony',\n",
       " 'anthonys',\n",
       " 'anthrax',\n",
       " 'anti',\n",
       " 'antiblight',\n",
       " 'antichrist',\n",
       " 'antifeminist',\n",
       " 'antioch',\n",
       " 'antiochhickoryhollow',\n",
       " 'antiochus',\n",
       " 'antipozi',\n",
       " 'antiterrorism',\n",
       " 'antonio',\n",
       " 'antony',\n",
       " 'ants',\n",
       " 'anu',\n",
       " 'anxiety',\n",
       " 'anxietyproblems',\n",
       " 'anxious',\n",
       " 'anybody',\n",
       " 'anymore',\n",
       " 'anytime',\n",
       " 'anyways',\n",
       " 'anza',\n",
       " 'aogashima',\n",
       " 'aoms',\n",
       " 'ap',\n",
       " 'apart',\n",
       " 'apartment',\n",
       " 'apartments',\n",
       " 'apaz',\n",
       " 'apc',\n",
       " 'apcaepdp',\n",
       " 'apch',\n",
       " 'apd',\n",
       " 'apga',\n",
       " 'apiece',\n",
       " 'apocalpytic',\n",
       " 'apocalypse',\n",
       " 'apocalyptic',\n",
       " 'apollo',\n",
       " 'apologies',\n",
       " 'apologise',\n",
       " 'apologize',\n",
       " 'apologized',\n",
       " 'app',\n",
       " 'appalling',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appeals',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appease',\n",
       " 'apperception',\n",
       " 'appetite',\n",
       " 'applaud',\n",
       " 'apple',\n",
       " 'applications',\n",
       " 'applied',\n",
       " 'applies',\n",
       " 'apply',\n",
       " 'appointment',\n",
       " 'appoints',\n",
       " 'appraisal',\n",
       " 'appreciate',\n",
       " 'appreciated',\n",
       " 'appreciativeinquiry',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'approaching',\n",
       " 'appropriate',\n",
       " 'appropriation',\n",
       " 'approval',\n",
       " 'approves',\n",
       " 'appx',\n",
       " 'appys',\n",
       " 'apr',\n",
       " 'april',\n",
       " 'apropos',\n",
       " 'apt',\n",
       " 'apts',\n",
       " 'apunk',\n",
       " 'aq',\n",
       " 'aqua',\n",
       " 'aquarium',\n",
       " 'aquarius',\n",
       " 'ar',\n",
       " 'ara',\n",
       " 'arab',\n",
       " 'arabia',\n",
       " 'arabian',\n",
       " 'arabic',\n",
       " 'arachys',\n",
       " 'arcade',\n",
       " 'arceen',\n",
       " 'archetype',\n",
       " 'archipelagowolves',\n",
       " 'architect',\n",
       " 'architects',\n",
       " 'architecture',\n",
       " 'area',\n",
       " 'areal',\n",
       " 'areas',\n",
       " 'areasminor',\n",
       " 'arena',\n",
       " 'arenanone',\n",
       " 'arent',\n",
       " 'areva',\n",
       " 'arfur',\n",
       " 'argentina',\n",
       " 'argentinean',\n",
       " 'argh',\n",
       " 'argsuppose',\n",
       " 'argue',\n",
       " 'argues',\n",
       " 'argument',\n",
       " 'ari',\n",
       " 'arian',\n",
       " 'ariana',\n",
       " 'arin',\n",
       " 'aris',\n",
       " 'ariz',\n",
       " 'arizona',\n",
       " 'arkan',\n",
       " 'arkansas',\n",
       " 'arlington',\n",
       " 'arm',\n",
       " 'armageddon',\n",
       " 'armed',\n",
       " 'armenians',\n",
       " 'armory',\n",
       " 'arms',\n",
       " 'army',\n",
       " 'armys',\n",
       " 'arnhem',\n",
       " 'arnley',\n",
       " 'arranged',\n",
       " 'arreat',\n",
       " 'arrest',\n",
       " 'arrested',\n",
       " 'arrestpastornganga',\n",
       " 'arrests',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'arrives',\n",
       " 'arriving',\n",
       " 'arrogant',\n",
       " 'ars',\n",
       " 'arse',\n",
       " 'arsenal',\n",
       " 'arsenals',\n",
       " 'arson',\n",
       " 'arsonist',\n",
       " 'arsonists',\n",
       " 'art',\n",
       " 'artectura',\n",
       " 'arti',\n",
       " 'articals',\n",
       " 'article',\n",
       " 'articles',\n",
       " 'artificial',\n",
       " 'artillery',\n",
       " 'artist',\n",
       " 'artisteoftheweekfact',\n",
       " 'artists',\n",
       " 'artistsunited',\n",
       " 'arts',\n",
       " 'artwork',\n",
       " 'arwx',\n",
       " 'ary',\n",
       " 'asae',\n",
       " 'asap',\n",
       " 'asb',\n",
       " 'asbury',\n",
       " 'ascend',\n",
       " 'aseer',\n",
       " 'asf',\n",
       " 'ash',\n",
       " 'ashayo',\n",
       " 'ashdod',\n",
       " 'ashenforest',\n",
       " 'ashes',\n",
       " 'ashesashes',\n",
       " 'ashestoashes',\n",
       " 'ashley',\n",
       " 'ashrafiyah',\n",
       " 'ashville',\n",
       " 'asia',\n",
       " 'asian',\n",
       " 'asics',\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'askcharley',\n",
       " 'askconnor',\n",
       " 'asked',\n",
       " 'askforalaska',\n",
       " 'askhcz',\n",
       " 'askin',\n",
       " 'asking',\n",
       " 'asks',\n",
       " 'asleep',\n",
       " 'aspect',\n",
       " 'aspects',\n",
       " 'asphalt',\n",
       " 'aspiring',\n",
       " 'ass',\n",
       " 'assad',\n",
       " 'assailant',\n",
       " 'assassins',\n",
       " 'assault',\n",
       " 'assembly',\n",
       " 'assertative',\n",
       " 'asses',\n",
       " 'assessment',\n",
       " 'assets',\n",
       " 'asshole',\n",
       " 'assholes',\n",
       " 'assistance',\n",
       " 'assistant',\n",
       " 'assisting',\n",
       " 'assnchat',\n",
       " 'associated',\n",
       " 'association',\n",
       " 'assume',\n",
       " 'assumes',\n",
       " 'assured',\n",
       " 'asswipe',\n",
       " 'astonishing',\n",
       " 'astounding',\n",
       " 'astrakhan',\n",
       " 'astrologian',\n",
       " 'astrology',\n",
       " 'astroturfers',\n",
       " 'asylum',\n",
       " 'ataecinema',\n",
       " 'atamathon',\n",
       " 'atc',\n",
       " 'atcha',\n",
       " 'ate',\n",
       " 'athens',\n",
       " 'athlete',\n",
       " 'athletics',\n",
       " 'atk',\n",
       " 'atl',\n",
       " 'atlanta',\n",
       " 'atlantic',\n",
       " 'atlas',\n",
       " 'atleast',\n",
       " 'atm',\n",
       " 'atmosphere',\n",
       " 'atmospheric',\n",
       " 'atom',\n",
       " 'atombomb',\n",
       " 'atomic',\n",
       " 'atomicbomb',\n",
       " 'attached',\n",
       " 'attack',\n",
       " 'attacked',\n",
       " 'attacking',\n",
       " 'attacks',\n",
       " 'attackshare',\n",
       " 'attained',\n",
       " 'attempt',\n",
       " 'attempted',\n",
       " 'attempting',\n",
       " 'attend',\n",
       " 'attendance',\n",
       " 'attended',\n",
       " 'attendees',\n",
       " 'attending',\n",
       " 'attention',\n",
       " 'attic',\n",
       " 'attila',\n",
       " 'attitude',\n",
       " 'attraction',\n",
       " 'attractive',\n",
       " 'atv',\n",
       " 'au',\n",
       " 'aubrey',\n",
       " 'auburn',\n",
       " 'auc',\n",
       " 'auckland',\n",
       " 'auction',\n",
       " 'auctions',\n",
       " 'audi',\n",
       " 'audience',\n",
       " 'audiences',\n",
       " 'audio',\n",
       " 'audit',\n",
       " 'aug',\n",
       " 'august',\n",
       " 'aul',\n",
       " 'aunt',\n",
       " 'aurora',\n",
       " 'aus',\n",
       " 'auspol',\n",
       " 'aussie',\n",
       " 'aussies',\n",
       " 'aust',\n",
       " 'austin',\n",
       " 'australia',\n",
       " 'australian',\n",
       " 'australians',\n",
       " 'australias',\n",
       " 'austrian',\n",
       " 'auth',\n",
       " 'authentic',\n",
       " 'authenticating',\n",
       " 'author',\n",
       " 'authorities',\n",
       " 'authors',\n",
       " 'autism',\n",
       " 'autismawareness',\n",
       " 'autistic',\n",
       " 'auto',\n",
       " 'autobiography',\n",
       " 'autobody',\n",
       " 'autoinsurance',\n",
       " 'automatic',\n",
       " 'automation',\n",
       " 'autumn',\n",
       " 'auz',\n",
       " 'av',\n",
       " 'ava',\n",
       " 'available',\n",
       " 'avalanche',\n",
       " 'avalanches',\n",
       " 'ave',\n",
       " 'avenged',\n",
       " 'avengers',\n",
       " 'avenue',\n",
       " 'average',\n",
       " 'avert',\n",
       " 'averted',\n",
       " 'avi',\n",
       " 'aviation',\n",
       " 'aviationaddicts',\n",
       " 'avigdorliberman',\n",
       " 'avoid',\n",
       " 'avoided',\n",
       " 'avoiding',\n",
       " 'avoids',\n",
       " 'avysss',\n",
       " 'aw',\n",
       " 'await',\n",
       " 'awaits',\n",
       " 'awake',\n",
       " 'awakenings',\n",
       " 'awards',\n",
       " 'aware',\n",
       " 'awareness',\n",
       " 'awash',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awesomeee',\n",
       " 'awesomelove',\n",
       " 'awesomesauce',\n",
       " 'awful',\n",
       " 'awkward',\n",
       " 'awn',\n",
       " 'awol',\n",
       " 'awww',\n",
       " 'ay',\n",
       " 'ayekoradio',\n",
       " 'ayhhhdjjfjrjjrdjjeks',\n",
       " 'ayyo',\n",
       " 'ayyy',\n",
       " 'az',\n",
       " 'azusa',\n",
       " 'azwx',\n",
       " 'ba',\n",
       " 'baaack',\n",
       " 'baan',\n",
       " 'babality',\n",
       " 'babalmao',\n",
       " 'babe',\n",
       " 'babes',\n",
       " 'babies',\n",
       " 'babri',\n",
       " 'baby',\n",
       " 'bachmann',\n",
       " 'background',\n",
       " 'backing',\n",
       " 'backlash',\n",
       " 'backpack',\n",
       " 'backroom',\n",
       " 'backs',\n",
       " 'backtoback',\n",
       " 'backty',\n",
       " 'backup',\n",
       " 'backups',\n",
       " 'backyard',\n",
       " 'backyards',\n",
       " 'bacteria',\n",
       " 'bad',\n",
       " 'badass',\n",
       " 'badchoices',\n",
       " 'badge',\n",
       " 'badges',\n",
       " 'badirandeal',\n",
       " 'badkitty',\n",
       " 'badly',\n",
       " 'badotweet',\n",
       " 'badu',\n",
       " 'bae',\n",
       " 'baekhyun',\n",
       " 'baffles',\n",
       " 'baffling',\n",
       " 'bag',\n",
       " 'baggage',\n",
       " 'bagged',\n",
       " 'bagging',\n",
       " 'bago',\n",
       " 'bags',\n",
       " 'bah',\n",
       " 'bahrain',\n",
       " 'bail',\n",
       " 'bailed',\n",
       " 'bailout',\n",
       " 'bairstow',\n",
       " 'bait',\n",
       " 'bajrangi',\n",
       " 'bak',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'bakeofffriends',\n",
       " 'baking',\n",
       " 'bal',\n",
       " 'balance',\n",
       " 'balanced',\n",
       " 'balcony',\n",
       " 'balding',\n",
       " 'bali',\n",
       " 'ball',\n",
       " 'ballews',\n",
       " 'balls',\n",
       " 'baltimore',\n",
       " 'bama',\n",
       " 'bamenda',\n",
       " 'ban',\n",
       " 'banana',\n",
       " 'bancodeseries',\n",
       " 'band',\n",
       " 'bandolier',\n",
       " 'bands',\n",
       " 'banerjee',\n",
       " 'bang',\n",
       " 'bangalore',\n",
       " 'bangin',\n",
       " 'bangladesh',\n",
       " 'bangladeshaffected',\n",
       " 'bangladeshflood',\n",
       " 'bangtan',\n",
       " 'bank',\n",
       " 'banki',\n",
       " 'banking',\n",
       " 'banks',\n",
       " 'bankstown',\n",
       " 'banksy',\n",
       " 'banned',\n",
       " 'bannister',\n",
       " 'bannukes',\n",
       " 'banquet',\n",
       " 'bans',\n",
       " 'banthebomb',\n",
       " 'bantrophyhunting',\n",
       " 'bar',\n",
       " 'barack',\n",
       " 'barak',\n",
       " 'barbados',\n",
       " 'barbaric',\n",
       " 'barber',\n",
       " 'barcelona',\n",
       " 'barcousky',\n",
       " 'bard',\n",
       " 'bardo',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'barfield',\n",
       " 'bargain',\n",
       " 'bark',\n",
       " 'barkevious',\n",
       " 'barking',\n",
       " 'barn',\n",
       " 'barra',\n",
       " 'barracks',\n",
       " 'barrier',\n",
       " 'barring',\n",
       " 'barrington',\n",
       " 'barry',\n",
       " 'bars',\n",
       " 'bartender',\n",
       " 'baruch',\n",
       " 'basalt',\n",
       " 'base',\n",
       " 'baseball',\n",
       " 'based',\n",
       " 'baseman',\n",
       " 'basement',\n",
       " 'bash',\n",
       " 'bashes',\n",
       " 'bashing',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'basics',\n",
       " 'basis',\n",
       " 'bask',\n",
       " 'baskets',\n",
       " 'bass',\n",
       " 'bastard',\n",
       " 'bastards',\n",
       " ...]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(train.clean_text, train.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8161523309258043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(test.clean_text)\n",
    "np.mean(predicted == test.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                                   alpha=1e-3, random_state=42)),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_svm = text_clf_svm.fit(train.clean_text, train.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7701904136572554"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_svm = text_clf_svm.predict(test.clean_text)\n",
    "np.mean(predicted_svm == test.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(train.clean_text, train.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788177339901478"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_improved = Pipeline([('vect', CountVectorizer()),\n",
    "                              ('clf', MultinomialNB()),\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf_improved.fit(train.clean_text, train.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8049901510177282"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf_improved.predict(test.clean_text)\n",
    "np.mean(predicted == test.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>aaalll</th>\n",
       "      <th>aaemiddleaged</th>\n",
       "      <th>aal</th>\n",
       "      <th>aan</th>\n",
       "      <th>aannnd</th>\n",
       "      <th>aar</th>\n",
       "      <th>aashiqui</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zotar</th>\n",
       "      <th>zouma</th>\n",
       "      <th>zrnf</th>\n",
       "      <th>zss</th>\n",
       "      <th>zumiez</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6086</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6087</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6088</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6089</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6090 rows × 12588 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       aa               aaaand               aaalll  \\\n",
       "0                    0.00                 0.00                 0.00   \n",
       "1                    0.00                 0.00                 0.00   \n",
       "2                    0.00                 0.00                 0.00   \n",
       "3                    0.00                 0.00                 0.00   \n",
       "4                    0.00                 0.00                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "6085                 0.00                 0.00                 0.00   \n",
       "6086                 0.00                 0.00                 0.00   \n",
       "6087                 0.00                 0.00                 0.00   \n",
       "6088                 0.00                 0.00                 0.00   \n",
       "6089                 0.00                 0.00                 0.00   \n",
       "\n",
       "            aaemiddleaged                  aal                  aan  \\\n",
       "0                    0.00                 0.00                 0.00   \n",
       "1                    0.00                 0.00                 0.00   \n",
       "2                    0.00                 0.00                 0.00   \n",
       "3                    0.00                 0.00                 0.00   \n",
       "4                    0.00                 0.00                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "6085                 0.00                 0.00                 0.00   \n",
       "6086                 0.00                 0.00                 0.00   \n",
       "6087                 0.00                 0.00                 0.00   \n",
       "6088                 0.00                 0.00                 0.00   \n",
       "6089                 0.00                 0.00                 0.00   \n",
       "\n",
       "                   aannnd                  aar             aashiqui  \\\n",
       "0                    0.00                 0.00                 0.00   \n",
       "1                    0.00                 0.00                 0.00   \n",
       "2                    0.00                 0.00                 0.00   \n",
       "3                    0.00                 0.00                 0.00   \n",
       "4                    0.00                 0.00                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "6085                 0.00                 0.00                 0.00   \n",
       "6086                 0.00                 0.00                 0.00   \n",
       "6087                 0.00                 0.00                 0.00   \n",
       "6088                 0.00                 0.00                 0.00   \n",
       "6089                 0.00                 0.00                 0.00   \n",
       "\n",
       "                       ab  ...                 zone                zones  \\\n",
       "0                    0.00  ...                 0.00                 0.00   \n",
       "1                    0.00  ...                 0.00                 0.00   \n",
       "2                    0.00  ...                 0.00                 0.00   \n",
       "3                    0.00  ...                 0.00                 0.00   \n",
       "4                    0.00  ...                 0.00                 0.00   \n",
       "...                   ...  ...                  ...                  ...   \n",
       "6085                 0.00  ...                 0.00                 0.00   \n",
       "6086                 0.00  ...                 0.00                 0.00   \n",
       "6087                 0.00  ...                 0.00                 0.00   \n",
       "6088                 0.00  ...                 0.00                 0.00   \n",
       "6089                 0.00  ...                 0.00                 0.00   \n",
       "\n",
       "                     zoom                zotar                zouma  \\\n",
       "0                    0.00                 0.00                 0.00   \n",
       "1                    0.00                 0.00                 0.00   \n",
       "2                    0.00                 0.00                 0.00   \n",
       "3                    0.00                 0.00                 0.00   \n",
       "4                    0.00                 0.00                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "6085                 0.00                 0.00                 0.00   \n",
       "6086                 0.00                 0.00                 0.00   \n",
       "6087                 0.00                 0.00                 0.00   \n",
       "6088                 0.00                 0.00                 0.00   \n",
       "6089                 0.00                 0.00                 0.00   \n",
       "\n",
       "                     zrnf                  zss               zumiez  \\\n",
       "0                    0.00                 0.00                 0.00   \n",
       "1                    0.00                 0.00                 0.00   \n",
       "2                    0.00                 0.00                 0.00   \n",
       "3                    0.00                 0.00                 0.00   \n",
       "4                    0.00                 0.00                 0.00   \n",
       "...                   ...                  ...                  ...   \n",
       "6085                 0.00                 0.00                 0.00   \n",
       "6086                 0.00                 0.00                 0.00   \n",
       "6087                 0.00                 0.00                 0.00   \n",
       "6088                 0.00                 0.00                 0.00   \n",
       "6089                 0.00                 0.00                 0.00   \n",
       "\n",
       "                   zurich                  zzz  \n",
       "0                    0.00                 0.00  \n",
       "1                    0.00                 0.00  \n",
       "2                    0.00                 0.00  \n",
       "3                    0.00                 0.00  \n",
       "4                    0.00                 0.00  \n",
       "...                   ...                  ...  \n",
       "6085                 0.00                 0.00  \n",
       "6086                 0.00                 0.00  \n",
       "6087                 0.00                 0.00  \n",
       "6088                 0.00                 0.00  \n",
       "6089                 0.00                 0.00  \n",
       "\n",
       "[6090 rows x 12588 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(train.clean_text)\n",
    "tf_idf_train = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just happened  terrible car crash</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>34</td>\n",
       "      <td>4.83</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>3</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "      <td>61</td>\n",
       "      <td>5.89</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is  forest fire at spot pond geese are f...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>94</td>\n",
       "      <td>4.00</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>3</td>\n",
       "      <td>#spokane #wildfires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>37</td>\n",
       "      <td>8.50</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "      <td>42</td>\n",
       "      <td>5.14</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_original keyword_original location_original  \\\n",
       "0            0              NaN               NaN   \n",
       "1            2              NaN               NaN   \n",
       "2            3              NaN               NaN   \n",
       "3            9              NaN               NaN   \n",
       "4           11              NaN               NaN   \n",
       "\n",
       "                                       text_original  special_chars_count  \\\n",
       "0                  just happened  terrible car crash                    0   \n",
       "1  heard about earthquake is different cities sta...                    3   \n",
       "2  there is  forest fire at spot pond geese are f...                    2   \n",
       "3              apocalypse lighting spokane wildfires                    3   \n",
       "4         typhoon soudelor kills in china and taiwan                    0   \n",
       "\n",
       "               hashtags labels  hashtags_count  labels_count  num_chars_count  \\\n",
       "0                   NaN    NaN               0             0                0   \n",
       "1          #earthquake     NaN               1             0                0   \n",
       "2                   NaN    NaN               0             0                0   \n",
       "3  #spokane #wildfires     NaN               2             0                0   \n",
       "4                   NaN    NaN               0             0                2   \n",
       "\n",
       "   links_count                                         clean_text  \\\n",
       "0            0                        happened terrible car crash   \n",
       "1            0  heard earthquake different cities stay safe ev...   \n",
       "2            0  forest fire spot pond geese fleeing across str...   \n",
       "3            0              apocalypse lighting spokane wildfires   \n",
       "4            0                typhoon soudelor kills china taiwan   \n",
       "\n",
       "   text_length     mean_word_length  vowels_count  short_words_count  \\\n",
       "0           34                 4.83            10                  2   \n",
       "1           61                 5.89            24                  1   \n",
       "2           94                 4.00            31                  7   \n",
       "3           37                 8.50            12                  0   \n",
       "4           42                 5.14            14                  2   \n",
       "\n",
       "   stopwords_count  words_count  \n",
       "0                2            6  \n",
       "1                2            9  \n",
       "2                9           19  \n",
       "3                0            4  \n",
       "4                2            7  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_featured.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(twt_data.clean_text, twt_data.target_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-1d476724229f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Datos/datos/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    220\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "predicted = text_clf.predict(test_data.clean_text)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>what if</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id_original keyword_original location_original text_original  \\\n",
       "13           43              NaN               NaN       what if   \n",
       "\n",
       "    special_chars_count hashtags labels  hashtags_count  labels_count  \\\n",
       "13                    2      NaN    NaN               0             0   \n",
       "\n",
       "    num_chars_count  links_count clean_text  text_length     mean_word_length  \\\n",
       "13                0            0        NaN            7                 3.00   \n",
       "\n",
       "    vowels_count  short_words_count  stopwords_count  words_count  \n",
       "13             2                  1                2            2  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[test_data.clean_text != test_data.clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['clean_text'].fillna(\" \", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[test_data.clean_text != test_data.clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['target'] = predicted\n",
    "test_data[['id_original', 'target']].rename(columns={'id_original': 'id'}).to_csv('~/Documents/Datos/DataSets/TP2/res_NB_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
