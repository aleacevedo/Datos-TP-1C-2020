{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importacion general de librerias y de visualizacion (matplotlib y seaborn)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "\n",
    "pd.options.display.float_format = '{:20,.2f}'.format # suprimimos la notacion cientifica en los outputs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/train_super_featured.csv')\n",
    "test_data = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_super_featured.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['keyword_original'].fillna('no_keyword', inplace=True)\n",
    "test_data['keyword_original'].fillna('no_keyword', inplace=True)\n",
    "train_data['location_original'].fillna('no_location', inplace=True)\n",
    "test_data['location_original'].fillna('no_location', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_original'].fillna('', inplace=True)\n",
    "train_data['clean_text'].fillna('', inplace=True)\n",
    "train_data['super_clean_text'].fillna('', inplace=True)\n",
    "train_data['kaggle_text'].fillna('', inplace=True)\n",
    "train_data['semi_cleaned_text'].fillna('', inplace=True)\n",
    "test_data['text_original'].fillna('', inplace=True)\n",
    "test_data['clean_text'].fillna('', inplace=True)\n",
    "test_data['super_clean_text'].fillna('', inplace=True)\n",
    "test_data['kaggle_text'].fillna('', inplace=True)\n",
    "test_data['semi_cleaned_text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import time\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.constraints import max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x12cdbae48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()            \n",
    "tokenizer.fit_on_texts(train_data['super_clean_text'].values.tolist())\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embeddings = {}\n",
    "with open('../../../DataSets/TP2/glove.840B.300d.txt', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        values = line.replace(\"\\n\", \"\").split(\" \")\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(X):\n",
    "    \n",
    "    tweets = X.apply(lambda s: s.split()).values      \n",
    "    vocab = {}\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        for word in tweet:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1                \n",
    "    return vocab\n",
    "\n",
    "\n",
    "def check_embeddings_coverage(X, embeddings):\n",
    "    \n",
    "    vocab = build_vocab(X)    \n",
    "    \n",
    "    covered = {}\n",
    "    oov = {}    \n",
    "    n_covered = 0\n",
    "    n_oov = 0\n",
    "    \n",
    "    for word in vocab:\n",
    "        try:\n",
    "            covered[word] = embeddings[word]\n",
    "            n_covered += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            n_oov += vocab[word]\n",
    "            \n",
    "    vocab_coverage = len(covered) / len(vocab)\n",
    "    text_coverage = (n_covered / (n_covered + n_oov))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    return sorted_oov, vocab_coverage, text_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 51.90% of vocabulary and 81.71% of text in Training Set text_original\n",
      "GloVe Embeddings cover 57.12% of vocabulary and 81.00% of text in Test Set text_original\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['text_original'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['text_original'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set text_original'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set text_original'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 85.76% of vocabulary and 95.89% of text in Training Set clean_text\n",
      "GloVe Embeddings cover 88.06% of vocabulary and 95.66% of text in Test Set clean_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['clean_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['clean_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set clean_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set clean_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 67.95% of vocabulary and 94.84% of text in Training Set kaggle_text\n",
      "GloVe Embeddings cover 75.66% of vocabulary and 94.99% of text in Test Set kaggle_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['kaggle_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['kaggle_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set kaggle_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set kaggle_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Embeddings cover 86.11% of vocabulary and 96.09% of text in Training Set super_clean_text\n",
      "GloVe Embeddings cover 89.24% of vocabulary and 96.12% of text in Test Set super_clean_text\n"
     ]
    }
   ],
   "source": [
    "train_glove_oov, train_glove_vocab_coverage, train_glove_text_coverage = check_embeddings_coverage(train_data['super_clean_text'], glove_embeddings)\n",
    "test_glove_oov, test_glove_vocab_coverage, test_glove_text_coverage = check_embeddings_coverage(test_data['super_clean_text'], glove_embeddings)\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Training Set super_clean_text'.format(train_glove_vocab_coverage, train_glove_text_coverage))\n",
    "print('GloVe Embeddings cover {:.2%} of vocabulary and {:.2%} of text in Test Set super_clean_text'.format(test_glove_vocab_coverage, test_glove_text_coverage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(list(tokenizer.word_index)) + 1\n",
    "embedding_size = 300\n",
    "embedding_matrix = np.random.uniform(-1, 1, (num_words, embedding_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i < num_words:\n",
    "        embedding_vector = glove_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "SEED = 1337\n",
    "skf = StratifiedKFold(n_splits=K, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_Glove:\n",
    "    def __init__(self, epochs, batch_size, num_filters, tokenizer, embedding_matrix, embedding_size):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_filters = num_filters\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.embedding_size = embedding_size\n",
    "        self.trained_model = []\n",
    "\n",
    "        \n",
    "    def encode(self, train_text, test_text):\n",
    "        x_train_tokens = tokenizer.texts_to_sequences(train_text)\n",
    "        x_test_tokens = tokenizer.texts_to_sequences(test_text)\n",
    "        num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "        num_tokens = np.array(num_tokens)\n",
    "        max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "        self.max_tokens = int(max_tokens)\n",
    "        x_train_pad = pad_sequences(x_train_tokens, maxlen=self.max_tokens)\n",
    "        x_test_pad = pad_sequences(x_test_tokens, maxlen=self.max_tokens)\n",
    "        return x_train_pad, x_test_pad\n",
    "    \n",
    "    def encode_fit(self, text):\n",
    "        x_final_test_tokens = tokenizer.texts_to_sequences(text)\n",
    "        x_final_test_pad = pad_sequences(x_final_test_tokens, maxlen=self.max_tokens)\n",
    "        return x_final_test_pad\n",
    "    \n",
    "    def build_model(self):\n",
    "        sequence_length = self.max_tokens\n",
    "        vocabulary_size = len(list(self.tokenizer.word_index)) + 1\n",
    "        embedding_dim = self.embedding_size\n",
    "        drop = 0.7\n",
    "        weight_decay = 1e-2\n",
    "        num_classes = 2\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=num_words,\n",
    "                            output_dim=embedding_size,\n",
    "                            weights= [self.embedding_matrix],\n",
    "                            input_length=self.max_tokens,        \n",
    "                            trainable=True,              #the layer is trained\n",
    "                            name='embedding_layer'))\n",
    "        model.add(Conv1D(self.num_filters, 7, activation='sigmoid', padding='same', kernel_constraint=max_norm(3), bias_constraint=max_norm(3)))\n",
    "        model.add(MaxPooling1D(2))\n",
    "        model.add(Conv1D(self.num_filters, 7, activation='sigmoid', padding='same', kernel_constraint=max_norm(3), bias_constraint=max_norm(3)))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dropout(drop))\n",
    "        model.add(Dense(8, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Dense(num_classes, activation='softmax'))  #multi-label (k-hot encoding)\n",
    "        adam = Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0005)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        return model\n",
    "        \n",
    "    def train(self, X):\n",
    "        for fold, (trn_idx, val_idx) in enumerate(skf.split(X['super_clean_text'], X['keyword_original'])):\n",
    "            \n",
    "            print('\\nFold {}\\n'.format(fold))\n",
    "        \n",
    "            X_trn_encoded, X_val_encoded = self.encode(X.loc[trn_idx, 'super_clean_text'].str.lower(),\n",
    "                                                       X.loc[val_idx, 'super_clean_text'].str.lower())\n",
    "            y_trn = X.loc[trn_idx, 'target_relabeled']\n",
    "            y_val = X.loc[val_idx, 'target_relabeled']\n",
    "            \n",
    "\n",
    "            # Model\n",
    "            model = self.build_model()        \n",
    "            \n",
    "            #define callbacks\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=8, verbose=1)\n",
    "            callbacks_list = [early_stopping]\n",
    "\n",
    "            model.fit(X_trn_encoded, y_trn, validation_data=(X_val_encoded, y_val), batch_size=self.batch_size, epochs=self.epochs, callbacks=callbacks_list, verbose=2)\n",
    "            \n",
    "            \n",
    "            self.trained_model.append(model)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        X_test_encoded = self.encode_fit(X['super_clean_text'].str.lower())\n",
    "        y_pred = np.zeros((X_test_encoded.shape[0], 2))\n",
    "\n",
    "        for model in self.trained_model:\n",
    "            y_pred += model.predict(X_test_encoded) / len(self.trained_model)\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TF_Glove(20, 128, 32, tokenizer, embedding_matrix, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0\n",
      "\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 16, 300)           4304100   \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 16, 32)            67232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 8, 32)             7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_26 (Glo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 4,378,814\n",
      "Trainable params: 4,378,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "30/30 - 2s - loss: 0.8839 - accuracy: 0.5457 - val_loss: 0.8161 - val_accuracy: 0.5516\n",
      "Epoch 2/20\n",
      "30/30 - 2s - loss: 0.8270 - accuracy: 0.5489 - val_loss: 0.8085 - val_accuracy: 0.5684\n",
      "Epoch 3/20\n",
      "30/30 - 2s - loss: 0.8037 - accuracy: 0.5604 - val_loss: 0.8015 - val_accuracy: 0.5708\n",
      "Epoch 4/20\n",
      "30/30 - 2s - loss: 0.7969 - accuracy: 0.5570 - val_loss: 0.7949 - val_accuracy: 0.5708\n",
      "Epoch 5/20\n",
      "30/30 - 2s - loss: 0.7759 - accuracy: 0.5793 - val_loss: 0.7884 - val_accuracy: 0.5724\n",
      "Epoch 6/20\n",
      "30/30 - 2s - loss: 0.7629 - accuracy: 0.6148 - val_loss: 0.7560 - val_accuracy: 0.7625\n",
      "Epoch 7/20\n",
      "30/30 - 2s - loss: 0.7207 - accuracy: 0.6576 - val_loss: 0.6946 - val_accuracy: 0.7791\n",
      "Epoch 8/20\n",
      "30/30 - 2s - loss: 0.6682 - accuracy: 0.7049 - val_loss: 0.6250 - val_accuracy: 0.7846\n",
      "Epoch 9/20\n",
      "30/30 - 2s - loss: 0.6082 - accuracy: 0.7514 - val_loss: 0.5710 - val_accuracy: 0.7901\n",
      "Epoch 10/20\n",
      "30/30 - 2s - loss: 0.5538 - accuracy: 0.7953 - val_loss: 0.5529 - val_accuracy: 0.7833\n",
      "Epoch 11/20\n",
      "30/30 - 2s - loss: 0.5055 - accuracy: 0.8216 - val_loss: 0.5228 - val_accuracy: 0.7985\n",
      "Epoch 12/20\n",
      "30/30 - 2s - loss: 0.4648 - accuracy: 0.8410 - val_loss: 0.5189 - val_accuracy: 0.8001\n",
      "Epoch 13/20\n",
      "30/30 - 2s - loss: 0.4255 - accuracy: 0.8723 - val_loss: 0.5189 - val_accuracy: 0.8004\n",
      "Epoch 14/20\n",
      "30/30 - 2s - loss: 0.3858 - accuracy: 0.8857 - val_loss: 0.5191 - val_accuracy: 0.8025\n",
      "Epoch 15/20\n",
      "30/30 - 2s - loss: 0.3536 - accuracy: 0.8986 - val_loss: 0.5310 - val_accuracy: 0.7996\n",
      "Epoch 16/20\n",
      "30/30 - 2s - loss: 0.3287 - accuracy: 0.9128 - val_loss: 0.5298 - val_accuracy: 0.8117\n",
      "Epoch 17/20\n",
      "30/30 - 2s - loss: 0.3083 - accuracy: 0.9188 - val_loss: 0.5488 - val_accuracy: 0.7998\n",
      "Epoch 18/20\n",
      "30/30 - 2s - loss: 0.2876 - accuracy: 0.9249 - val_loss: 0.5535 - val_accuracy: 0.8033\n",
      "Epoch 19/20\n",
      "30/30 - 2s - loss: 0.2628 - accuracy: 0.9351 - val_loss: 0.5653 - val_accuracy: 0.8040\n",
      "Epoch 20/20\n",
      "30/30 - 2s - loss: 0.2532 - accuracy: 0.9364 - val_loss: 0.5730 - val_accuracy: 0.8046\n",
      "Epoch 00020: early stopping\n",
      "\n",
      "Fold 1\n",
      "\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 16, 300)           4304100   \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 16, 32)            67232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 8, 32)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 8, 32)             7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_27 (Glo (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 4,378,814\n",
      "Trainable params: 4,378,814\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "30/30 - 2s - loss: 0.9111 - accuracy: 0.5359 - val_loss: 0.8113 - val_accuracy: 0.4714\n",
      "Epoch 2/20\n",
      "30/30 - 2s - loss: 0.8313 - accuracy: 0.5450 - val_loss: 0.8094 - val_accuracy: 0.4288\n",
      "Epoch 3/20\n",
      "30/30 - 2s - loss: 0.8031 - accuracy: 0.5508 - val_loss: 0.8023 - val_accuracy: 0.4288\n",
      "Epoch 4/20\n",
      "30/30 - 2s - loss: 0.7915 - accuracy: 0.5571 - val_loss: 0.7951 - val_accuracy: 0.4798\n",
      "Epoch 5/20\n",
      "30/30 - 2s - loss: 0.7800 - accuracy: 0.5697 - val_loss: 0.7826 - val_accuracy: 0.6487\n",
      "Epoch 6/20\n",
      "30/30 - 2s - loss: 0.7596 - accuracy: 0.5955 - val_loss: 0.7700 - val_accuracy: 0.6913\n",
      "Epoch 7/20\n",
      "30/30 - 2s - loss: 0.7367 - accuracy: 0.6215 - val_loss: 0.7439 - val_accuracy: 0.7622\n",
      "Epoch 8/20\n",
      "30/30 - 2s - loss: 0.7089 - accuracy: 0.6370 - val_loss: 0.6980 - val_accuracy: 0.7748\n",
      "Epoch 9/20\n",
      "30/30 - 2s - loss: 0.6683 - accuracy: 0.6985 - val_loss: 0.6309 - val_accuracy: 0.7924\n",
      "Epoch 10/20\n",
      "30/30 - 2s - loss: 0.6195 - accuracy: 0.7315 - val_loss: 0.5736 - val_accuracy: 0.7977\n",
      "Epoch 11/20\n",
      "30/30 - 2s - loss: 0.5785 - accuracy: 0.7568 - val_loss: 0.5392 - val_accuracy: 0.8035\n",
      "Epoch 12/20\n",
      "30/30 - 2s - loss: 0.5256 - accuracy: 0.7998 - val_loss: 0.5152 - val_accuracy: 0.8100\n",
      "Epoch 13/20\n",
      "30/30 - 2s - loss: 0.4849 - accuracy: 0.8308 - val_loss: 0.5009 - val_accuracy: 0.8111\n",
      "Epoch 14/20\n",
      "30/30 - 2s - loss: 0.4408 - accuracy: 0.8503 - val_loss: 0.4928 - val_accuracy: 0.8150\n",
      "Epoch 15/20\n",
      "30/30 - 2s - loss: 0.4037 - accuracy: 0.8666 - val_loss: 0.4945 - val_accuracy: 0.8171\n",
      "Epoch 16/20\n",
      "30/30 - 2s - loss: 0.3661 - accuracy: 0.8899 - val_loss: 0.5018 - val_accuracy: 0.8161\n",
      "Epoch 17/20\n",
      "30/30 - 2s - loss: 0.3375 - accuracy: 0.8913 - val_loss: 0.5116 - val_accuracy: 0.8163\n",
      "Epoch 18/20\n",
      "30/30 - 2s - loss: 0.3041 - accuracy: 0.9165 - val_loss: 0.5273 - val_accuracy: 0.8163\n",
      "Epoch 19/20\n",
      "30/30 - 2s - loss: 0.2862 - accuracy: 0.9199 - val_loss: 0.5494 - val_accuracy: 0.8092\n",
      "Epoch 20/20\n",
      "30/30 - 2s - loss: 0.2611 - accuracy: 0.9317 - val_loss: 0.5612 - val_accuracy: 0.8158\n"
     ]
    }
   ],
   "source": [
    "predicted = model.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(test_data)\n",
    "predicted =np.argmax(predicted, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_liked = pd.read_csv('~/Documents/Datos/DataSets/TP2/test_with_targets.csv', dtype={'id': np.int16, 'target': np.int8})\n",
    "df_liked.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8044744100520993"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.round(predicted).astype('int')\n",
    "np.mean(y_pred.flatten() == df_liked.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['target'] = np.round(y_pred.flatten()).astype('int')\n",
    "df_test[['id', 'target']].to_csv('~/Documents/Datos/DataSets/TP2/res_', index=False)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
