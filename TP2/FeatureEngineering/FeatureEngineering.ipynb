{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Documents/Datos/DataSets/TP2/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(x,char):\n",
    "    words = \"\"\n",
    "    for word in x:\n",
    "        if word.startswith(char):\n",
    "            words = words + word + \" \"\n",
    "    return words\n",
    "\n",
    "def count_vowels(x):\n",
    "    return (x.count('a') + x.count('e') + x.count('i') + x.count('o') + x.count('u'))\n",
    "\n",
    "def count_short_words(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if 1 <= len(word) <= 3:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_stopwords(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):        \n",
    "    clean_word = ''.join([char for char in word if char not in string.punctuation])\n",
    "    return clean_word\n",
    "\n",
    "def cleaning_text(text):\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text_tokenize = tokenizer.tokenize(text)\n",
    "    wordlist = []\n",
    "    for word in text_tokenize:\n",
    "        word = word.lower()\n",
    "        word = re.sub('(?P<url>https?://[^\\s]+)', ' ', word)\n",
    "        word = remove_punctuation(word)\n",
    "        word = re.sub(r'[^\\w]', ' ', word)\n",
    "        word = unidecode.unidecode(word)\n",
    "        word = re.sub(r'[0-9]','', word)\n",
    "        if((word != '')&(word != ' ')&(word not in stopwords)):\n",
    "            wordlist.append(word)\n",
    "        word = stemmer.stem(word)\n",
    "    clean_text = ' '.join(wordlist)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>target_label</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>semi_cleaned_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "      <td>69</td>\n",
       "      <td>4.384615</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>38</td>\n",
       "      <td>4.571429</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "      <td>133</td>\n",
       "      <td>5.090909</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#wildfires</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>people receive wildfires evacuation orders cal...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>65</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "      <td>88</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_original keyword_original location_original  \\\n",
       "0            1              NaN               NaN   \n",
       "1            4              NaN               NaN   \n",
       "2            5              NaN               NaN   \n",
       "3            6              NaN               NaN   \n",
       "4            7              NaN               NaN   \n",
       "\n",
       "                                       text_original  target_label  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...             1   \n",
       "1             Forest fire near La Ronge Sask. Canada             1   \n",
       "2  All residents asked to 'shelter in place' are ...             1   \n",
       "3  13,000 people receive #wildfires evacuation or...             1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...             1   \n",
       "\n",
       "   special_chars_count             hashtags labels  hashtags_count  \\\n",
       "0                    1         #earthquake                       1   \n",
       "1                    1                                           0   \n",
       "2                    3                                           0   \n",
       "3                    2          #wildfires                       1   \n",
       "4                    2  #alaska #wildfires                       2   \n",
       "\n",
       "   labels_count  num_chars_count  links_count  \\\n",
       "0             0                0            0   \n",
       "1             0                0            0   \n",
       "2             0                0            0   \n",
       "3             0                5            0   \n",
       "4             0                0            0   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0       deeds reason earthquake may allah forgive us   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  residents asked shelter place notified officer...   \n",
       "3  people receive wildfires evacuation orders cal...   \n",
       "4  got sent photo ruby alaska smoke wildfires pou...   \n",
       "\n",
       "                                   semi_cleaned_text  text_length  \\\n",
       "0  our deeds are the reason of this earthquake ma...           69   \n",
       "1              forest fire near la ronge sask canada           38   \n",
       "2  all residents asked to shelter in place are be...          133   \n",
       "3  people receive wildfires evacuation orders in ...           65   \n",
       "4  just got sent this photo from ruby alaska as s...           88   \n",
       "\n",
       "   mean_word_length  vowels_count  short_words_count  stopwords_count  \\\n",
       "0          4.384615            22                  7                5   \n",
       "1          4.571429            13                  1                0   \n",
       "2          5.090909            44                  9                9   \n",
       "3          7.125000            24                  1                1   \n",
       "4          4.500000            24                  3                6   \n",
       "\n",
       "   words_count  \n",
       "0           13  \n",
       "1            7  \n",
       "2           22  \n",
       "3            9  \n",
       "4           17  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"special_chars_count\"] =  train[\"text\"]\n",
    "train[\"special_chars_count\"] =  train[\"special_chars_count\"].str.lower()\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].str.strip()\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(' +','', x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(r'[0-9]','', x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].str.len()\n",
    "\n",
    "train[\"hashtags\"] = train[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'#'))\n",
    "train[\"labels\"] = train[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'@'))\n",
    "train[\"hashtags_count\"] = train[\"hashtags\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "train[\"labels_count\"] = train[\"labels\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "\n",
    "train[\"num_chars_count\"] = train[\"text\"]\n",
    "train[\"num_chars_count\"] =  train[\"num_chars_count\"].str.lower()\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].str.strip()\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].str.len()\n",
    "\n",
    "train[\"links_count\"] = train['text'].apply(lambda x: len([w for w in str(x).lower().split()\n",
    "                                                           if 'http' in w or 'https' in w]))\n",
    "\n",
    "train[\"clean_text\"] = train[\"text\"].apply(lambda x: cleaning_text(x)) # para el bag of words o tf-idf\n",
    "\n",
    "train[\"semi_cleaned_text\"] = train[\"text\"].str.lower()\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: re.sub('(?P<url>https?://[^\\s]+)', ' ', x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: re.sub(r'_', ' ', x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: re.sub(r'[0-9]',' ', x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: re.sub(' +',' ', x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].apply(lambda x: unidecode.unidecode(x))\n",
    "train[\"semi_cleaned_text\"] = train[\"semi_cleaned_text\"].str.strip()\n",
    "train[\"text_length\"] = train[\"text\"].str.len()\n",
    "\n",
    "train[\"mean_word_length\"] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "train[\"vowels_count\"] = train[\"text\"].apply(lambda x: count_vowels(x))\n",
    "train[\"short_words_count\"] = train[\"text\"].apply(lambda x: count_short_words(x))\n",
    "train[\"stopwords_count\"] = train[\"text\"].apply(lambda x: count_stopwords(x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(r'\\b\\w{1}\\b', '', x))\n",
    "train[\"words_count\"] = train[\"text\"].str.split(' ').apply(lambda x: len(x))\n",
    "\n",
    "train[\"keyword\"] = train[\"keyword\"].str.replace('%20',' ')\n",
    "train[\"keyword\"] = train[\"keyword\"].astype('category')\n",
    "\n",
    "train.rename(columns={\"target\":\"target_label\"}, inplace=True)\n",
    "# Si usamos BoW o TF-IDF\n",
    "train.rename(columns={\"location\":\"location_original\"}, inplace=True)\n",
    "train.rename(columns={\"id\":\"id_original\"}, inplace=True)\n",
    "train.rename(columns={\"text\":\"text_original\"}, inplace=True)\n",
    "train.rename(columns={\"keyword\":\"keyword_original\"}, inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('~/Documents/Datos/DataSets/TP2/train_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
