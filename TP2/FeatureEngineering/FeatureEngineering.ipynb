{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import SnowballStemmer\n",
    "import string\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('~/Documents/Datos/DataSets/TP2/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(x,char):\n",
    "    words = \"\"\n",
    "    for word in x:\n",
    "        if word.startswith(char):\n",
    "            words = words + word + \" \"\n",
    "    return words\n",
    "\n",
    "def count_vowels(x):\n",
    "    return (x.count('a') + x.count('e') + x.count('i') + x.count('o') + x.count('u'))\n",
    "\n",
    "def count_short_words(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if 1 <= len(word) <= 3:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def count_stopwords(x):\n",
    "    count = 0\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        if word in stopwords:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):        \n",
    "    clean_word = ''.join([char for char in word if char not in string.punctuation])\n",
    "    return clean_word\n",
    "\n",
    "def cleaning_text(text):\n",
    "    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    text_tokenize = tokenizer.tokenize(text)\n",
    "    wordlist = []\n",
    "    for word in text_tokenize:\n",
    "        word = word.lower()\n",
    "        word = re.sub('(?P<url>https?://[^\\s]+)', ' ', word)\n",
    "        word = remove_punctuation(word)\n",
    "        word = re.sub(r'[^\\w]', ' ', word)\n",
    "        word = unidecode.unidecode(word)\n",
    "        word = re.sub(r'[0-9]','', word)\n",
    "        if((word != '')&(word != ' ')&(word not in stopwords)):\n",
    "            wordlist.append(word)\n",
    "        word = stemmer.stem(word)\n",
    "    clean_text = ' '.join(wordlist)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_original</th>\n",
       "      <th>keyword_original</th>\n",
       "      <th>location_original</th>\n",
       "      <th>text_original</th>\n",
       "      <th>special_chars_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>labels</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>labels_count</th>\n",
       "      <th>num_chars_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>vowels_count</th>\n",
       "      <th>short_words_count</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>words_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just happened  terrible car crash</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>34</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard about earthquake is different cities sta...</td>\n",
       "      <td>3</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>heard earthquake different cities stay safe ev...</td>\n",
       "      <td>61</td>\n",
       "      <td>5.888889</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is  forest fire at spot pond geese are f...</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>94</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>3</td>\n",
       "      <td>#spokane #wildfires</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>37</td>\n",
       "      <td>8.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>typhoon soudelor kills china taiwan</td>\n",
       "      <td>42</td>\n",
       "      <td>5.142857</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_original keyword_original location_original  \\\n",
       "0            0              NaN               NaN   \n",
       "1            2              NaN               NaN   \n",
       "2            3              NaN               NaN   \n",
       "3            9              NaN               NaN   \n",
       "4           11              NaN               NaN   \n",
       "\n",
       "                                       text_original  special_chars_count  \\\n",
       "0                  just happened  terrible car crash                    0   \n",
       "1  heard about earthquake is different cities sta...                    3   \n",
       "2  there is  forest fire at spot pond geese are f...                    2   \n",
       "3              apocalypse lighting spokane wildfires                    3   \n",
       "4         typhoon soudelor kills in china and taiwan                    0   \n",
       "\n",
       "               hashtags labels  hashtags_count  labels_count  num_chars_count  \\\n",
       "0                                            0             0                0   \n",
       "1          #earthquake                       1             0                0   \n",
       "2                                            0             0                0   \n",
       "3  #spokane #wildfires                       2             0                0   \n",
       "4                                            0             0                2   \n",
       "\n",
       "   links_count                                         clean_text  \\\n",
       "0            0                        happened terrible car crash   \n",
       "1            0  heard earthquake different cities stay safe ev...   \n",
       "2            0  forest fire spot pond geese fleeing across str...   \n",
       "3            0              apocalypse lighting spokane wildfires   \n",
       "4            0                typhoon soudelor kills china taiwan   \n",
       "\n",
       "   text_length  mean_word_length  vowels_count  short_words_count  \\\n",
       "0           34          4.833333            10                  2   \n",
       "1           61          5.888889            24                  1   \n",
       "2           94          4.000000            31                  7   \n",
       "3           37          8.500000            12                  0   \n",
       "4           42          5.142857            14                  2   \n",
       "\n",
       "   stopwords_count  words_count  \n",
       "0                2            6  \n",
       "1                2            9  \n",
       "2                9           19  \n",
       "3                0            4  \n",
       "4                2            7  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"special_chars_count\"] =  train[\"text\"]\n",
    "train[\"special_chars_count\"] =  train[\"special_chars_count\"].str.lower()\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].str.strip()\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(' +','', x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].apply(lambda x: re.sub(r'[0-9]','', x))\n",
    "train[\"special_chars_count\"] = train[\"special_chars_count\"].str.len()\n",
    "\n",
    "train[\"hashtags\"] = train[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'#'))\n",
    "train[\"labels\"] = train[\"text\"].str.lower().str.split(' ').apply(lambda x: concatenate(x,'@'))\n",
    "train[\"hashtags_count\"] = train[\"hashtags\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "train[\"labels_count\"] = train[\"labels\"].str.split(' ').apply(lambda x: len(x))-1\n",
    "\n",
    "train[\"num_chars_count\"] = train[\"text\"]\n",
    "train[\"num_chars_count\"] =  train[\"num_chars_count\"].str.lower()\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].apply(lambda x: re.sub(r'[a-z]','',x))\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].apply(lambda x: re.sub(r'[^\\w]','',x))\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].str.strip()\n",
    "train[\"num_chars_count\"] = train[\"num_chars_count\"].str.len()\n",
    "\n",
    "train[\"links_count\"] = train['text'].apply(lambda x: len([w for w in str(x).lower().split()\n",
    "                                                           if 'http' in w or 'https' in w]))\n",
    "\n",
    "train[\"clean_text\"] = train[\"text\"].apply(lambda x: cleaning_text(x)) # para el bag of words o tf-idf\n",
    "\n",
    "train[\"text\"] = train[\"text\"].str.lower()\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub('(?P<url>https?://[^\\s]+)', ' ', x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(r'[^\\w]', ' ', x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(r'_', ' ', x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(r'[0-9]',' ', x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(' +',' ', x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: unidecode.unidecode(x))\n",
    "train[\"text\"] = train[\"text\"].str.strip()\n",
    "train[\"text_length\"] = train[\"text\"].str.len()\n",
    "\n",
    "train[\"mean_word_length\"] = train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "train[\"vowels_count\"] = train[\"text\"].apply(lambda x: count_vowels(x))\n",
    "train[\"short_words_count\"] = train[\"text\"].apply(lambda x: count_short_words(x))\n",
    "train[\"stopwords_count\"] = train[\"text\"].apply(lambda x: count_stopwords(x))\n",
    "train[\"text\"] = train[\"text\"].apply(lambda x: re.sub(r'\\b\\w{1}\\b', '', x))\n",
    "train[\"words_count\"] = train[\"text\"].str.split(' ').apply(lambda x: len(x))\n",
    "\n",
    "train[\"keyword\"] = train[\"keyword\"].str.replace('%20',' ')\n",
    "train[\"keyword\"] = train[\"keyword\"].astype('category')\n",
    "\n",
    "train.rename(columns={\"target\":\"target_label\"}, inplace=True)\n",
    "# Si usamos BoW o TF-IDF\n",
    "train.rename(columns={\"location\":\"location_original\"}, inplace=True)\n",
    "train.rename(columns={\"id\":\"id_original\"}, inplace=True)\n",
    "train.rename(columns={\"text\":\"text_original\"}, inplace=True)\n",
    "train.rename(columns={\"keyword\":\"keyword_original\"}, inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('~/Documents/Datos/DataSets/TP2/test_featured.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
